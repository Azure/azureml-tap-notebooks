{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AML Package for Text Analytics - Text Classification \n",
    "### Text Classification using Convolutional Neural Networks (CNNs) and Word2Vec Embeddings\n",
    "\n",
    "### Background\n",
    "\n",
    "Sentiment analysis is the task of extracting the intended subjective information conveyed by a text. It's most common form is for polarity extraction, i.e., distinguishing between positive and negative attitudes toward some item or product.\n",
    "\n",
    "One major difficulty is that the judgment of the polarity of an opinion can be subjective. Humans, for instance, usually disagree when judging the sentiment of a sentence, typically reaching only modest inter‚Äêannotator agreement.\n",
    "\n",
    "Moreover, polarity has to be interpreted within a context (contextual polarity). Thus, the polarity of a word can be modified in a sentence due to linguistic phenomena such as negation, modality, intensifiers, diminishers, and word sense.\n",
    "\n",
    "Deep learning is an emergent area of machine learning that offers methods for learning feature representation in a supervised or unsupervised fashion within a hierarchy. Typically, high layers of the hierarchy will have a more abstract representation (i.e., distributed representation) than lower layers. Higher layers evolve during training to exploit complex compositional nonlinear functions of the lower layers.\n",
    "\n",
    "We apply deep learning architectures to the task of sentiment analysis on a corpus of [polarized movie reviews](http://ai.stanford.edu/~amaas/data/sentiment/) from the IMDB website. This dataset was curated by Andrew Maas and fellow researchers at Stanford University, and is sometimes referred to as the _large movie review dataset_. We've provided a helper function to download the raw dataset from the URL, and transform it into a pandas DataFrame. \n",
    "\n",
    "\n",
    "### Convolutions for Text\n",
    "In this tutorial we showcase the use of neural networks with convolutions and pooling operations. The main idea behind a convolution and pooling architecture for language tasks is to apply a nonlinear (learned) function over each instantiation of a $k$-word sliding window over the sentence. For our use-case, we use a 1D convolution + maximum pooling over the review using a pre-trained word-embedding matrix.\n",
    "\n",
    "This architecture looks up each word in the embedding matrix to obtain an embedding vector, and a \"window\" of embedding vectors are concatenated together prior to the application of non-linearity and a maximum pooling operation to create features to use as predictors of sentiment. A pictorial representation of this operation over a sample sentence is given below (image is from _Yoav Goldberg (2017) - Neural Networks for Natural Language Processing, Morgan & Claypool Publishers_)\n",
    "\n",
    "![Yoav Goldberg - Neural Networks for Natural Language Processing](../imgs/conv-pool-text.png)\n",
    "\n",
    "Following are the steps for creating and deploying a custom entity extraction model using the package:\n",
    "<br> Step 1: Loading dataset and pre-trained embedding model\n",
    "<br> Step 2: Model training\n",
    "<br> Step 3: Apply the custom entity extractor \n",
    "<br> Step 4: Performance Evaluation\n",
    "<br> Step 5: Save the pipeline \n",
    "<br> Step 6: Load the pipeline \n",
    "<br> Step 7: Run the pipeline on unlabeled data\n",
    "<br> Step 8: Operationalize - Deploy the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: azureml-tatk\n",
      "Version: 0.1.18123.2b3\n",
      "Summary: Microsoft Azure Machine Learning Package for Text Analytics\n",
      "Home-page: https://microsoft.sharepoint.com/teams/TextAnalyticsPackagePreview\n",
      "Author: Microsoft Corporation\n",
      "Author-email: amltap@microsoft.com\n",
      "License: UNKNOWN\n",
      "Location: c:\\users\\alizaidi\\appdata\\local\\amlworkbench\\python\\lib\\site-packages\n",
      "Requires: numpy, matplotlib, pandas, bqplot, unidecode, azure-storage, ipywidgets, dill, pyspark, jsonpickle, docker, keras, nltk, pdfminer.six, pytest, azure-ml-api-sdk, nose, ipython, gensim, scikit-learn, ruamel.yaml, scipy, sklearn-crfsuite, lxml, validators, requests, h5py, qgrid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 9.0.3, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip show azureml-tatk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alizaidi\\AppData\\local\\AmlWorkbench\\Python\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Loading dataset and pre-trained embedding model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to download the movie review dataset as well as the embedding model we will use in our network. Please download the data from [IMDB Data](ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz) and save it to your home directory under `tatk\\resources\\data\\imdb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tatk.utils.load_data import load_imdb_data\n",
    "resources_dir = os.path.join(os.path.expanduser(\"~\"), \"tatk\", \"resources\")\n",
    "data_dir = os.path.join(resources_dir, \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start downloading text-classification from imdb\\movie_data.csv into C:\\Users\\alizaidi\\tatk\\resources\\data\\imdb\\movie_data.csv\n",
      "downloading finished successfully in 0.29 mins\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = load_imdb_data(data_dir=str(data_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Embedding Model\n",
    "\n",
    "Download pretrained Google word embedding model [GoogleNews-vectors-negative300.bin.gz](https://drive.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM&export=download), and save it your home directory under `tatk\\resources\\models\\Word2Vec_Models\\`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start downloading Word2Vec_Models/GoogleNews-vectors-negative300.bin from word-embedding into C:\\Users\\alizaidi\\tatk\\resources\\models\\Word2Vec_Models/GoogleNews-vectors-negative300.bin\n",
      "downloading finished successfully in 13.14 mins\n"
     ]
    }
   ],
   "source": [
    "from tatk.utils.load_data import models_dir, download_embedding_model\n",
    "embedding_file_path = download_embedding_model(models_dir, embedding_type=\"google\")\n",
    "import pathlib\n",
    "# embedding_file_path = str(pathlib.Path(resources_dir).joinpath(\"models\", \"Word2Vec_Models\", \n",
    "#                                                                \"GoogleNews-vectors-negative300.bin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I don't know if it was the directors intent to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This extremely low-budget monster flick center...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Angry Red Planet (Quickie Review) &lt;br /&gt;&lt;b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>One of the unsung gems of the 1980's, Scenes.....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I could not even bring myself to watch this mo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  I don't know if it was the directors intent to...          1\n",
       "1  This extremely low-budget monster flick center...          0\n",
       "2  The Angry Red Planet (Quickie Review) <br /><b...          0\n",
       "3  One of the unsung gems of the 1980's, Scenes.....          1\n",
       "4  I could not even bring myself to watch this mo...          0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_train = pd.DataFrame({'review': X_train, 'sentiment': y_train})\n",
    "imdb_train[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_train['word_counts'] = imdb_train['review'].str.split().apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x213c6ff7a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(imdb_train['word_counts'], bins = \"auto\")\n",
    "plt.title(\"count frequency of words per review\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average unique tokens in a review:  230.61768\n"
     ]
    }
   ],
   "source": [
    "print(\"average unique tokens in a review: \", imdb_train['word_counts'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEICAYAAAC0+DhzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAHBlJREFUeJzt3X2cXFWd5/HPlzSEZwKkQUgCAROQwLorZgBXdoYlLIToEHYWxvAaJGAkLoLDqKOCuhsEWWFGjeCIEE3GgJgQMyhReTDylPEhCc2ADEnANE9Jk5A05gEQQYO//eOeYi59qrsrXd1d6eT7fr36lXvPOffec25V17fuuVUdRQRmZmZlOzW6A2Zmtu1xOJiZWcbhYGZmGYeDmZllHA5mZpZxOJiZWcbhsAOSdJGkdZJekbR/o/uzLZG0m6QfSdos6fv9cLxXJB3e18fZlkh6QNKH++lYJ0lqq7Ht+ZJ+3sPj9HjbbVVToztgvUvSs8CHI+JnndTvDHwVOCEift2ffRsgzgIOBPaPiC19fbCI2LOvj2HWE75y2PEcCOwKLKtWKWlHf8NwKPCbWoPB58vnYHvlcOhDkkZIul1Su6TfSvqnVL6TpM9Lek7Sekk3S9on1WWXwZKelXRKWr5C0ry0zcuSlkkam+puAQ4BfpSmKz7dYT9HAE+m1U2S7kvlIeliSSuBlansHZIWStog6UlJf13az/6SFkh6SdJSSVdVLqkljUz7ayq1f8s0gqQPSVohaaOkeyQdWqoLSf9b0spU/w1JKtVfmLZ9WdJyScdK+pSkf+kw1q9L+lonj8tRqU+b0vk7I5V/Afi/wAfS+ZtSZdsrJM2X9F1JLwHnp8fzMklPpcd5nqT9Uvu7JV3SYR+/lvRXpfGOSsuDJX1Z0qo07XejpN1S3YOS/ldaPjFtNyGtnyLp0bQ8KrXdLOlFSbd1cg4qj9NUSWskrZX0yVJ9V2OqbDtF0irgvk6OMVHSo+l58pSk8VXavF3SfekYL0q6VdKQUv1nJD2fHu8nJY1L5cdJakn7Xifpq9X6UOV4lTFVnj//M2+ir6fz90TleKliH0kz07l6XtIXJQ2q5bgDUkT4pw9+gEHAr4HpwB4U79ZPTHUfAlqBw4E9gduBW1LdSUBbh309C5ySlq8AXgMmpGN8CVhcrW0n/RoJBNBUKgtgIbAfsFvq72rgAoqpx2OBF4GjU/u5wLzU7hjgeeDnXez/AYqpLoAz09iPSvv+PPDLDn35MTCEIujagfGp7ux0rD8DBIyieKd/EPA7YEhq1wSsB95dZfw7p+N/FtgFOBl4GTiydH6/28X5uwL4YxrHTul8/R2wGBgODAZuAuak9ucBvyhtPwbYBAwujXdUWv4asCA9DnsBPwK+lOquBL6elj8LPAVcW6q7Li3PAT6X+vbmc66L58Gc9Dj+p3SuK8+zrsZU2fbmtO1uVfZ/HLAZ+B+pL8OAd1R5PoxKbQYDzcAi4Gup7kiK5+HBpeO+PS3/CvhgWt6TYpq02jhPovT7RPEcOjj16QPpeXNQqjsf2AJ8PD1PPpDGsF+q/2E6D3sABwBLgY+Utv15o193evU1rNEd2F5/gPekX7amKnX3Ah8trR9J8YLT1PHJnOqf5a3h8LNS3Rjg99XadtKvyi92x3A4ubT+AeBfO2x3EzCNIpD+WPlFT3X/j9rD4S5gSqluJ+BV4NBSX04s1c8DLkvL9wCXdjKuu4AL0/L7geWdtPtvwAvATqWyOcAVpfPbXTgs6lC2AhhXWj+o9HjulV6AKuO7GpjV4dyPogi735Fe/ErPoWfS8jjgsbR8N/Bh0psC4EHgr9LyzcAMYHg3z8/K41R+HP8BmFnDmCrbHt7F/m8CpndS9+bzoUrdmcAjaXkURcifAuzcod0i4AvA0G7GeRIdfp861D8KTEzL5wNrAJXqlwIfpJiOfZ1SEALnAPeXtt2uwsHTSn1nBPBcVJ+7Phh4rrT+HMUv3YE17vuF0vKrwK6qf953dWn5UOD4NO2ySdIm4G+At1G8u2vq0L48lu4cClxX2u8GihfGYaU2HcdXuWk7guIdczWzgXPT8rnALZ20OxhYHRF/KpU91+H43VndYf1Q4AelMa0A3gAOjIiXgZ8Ak1LbScCtVfbZDOwOPFzaz92pHIp3ykdIOhD4LxQhMELSUIp36YtSu09TnM+lacrsQ1sxlucozk+XY+riPJR19Vi9SdIBkuamaZqXgO8CQwEiopXiCuYKYH1qV+nfFOAI4AlJD0l6f3fHSsc7L011VcZ1TOV4yfORXu2Tyjk5lOJqYm1p25soriC2Sw6HvrMaOKSTF+01FE+2ikMoLmfXUbx73L1SkeY0m6ldT//Mbnm71cCDETGk9LNnRFxEcTW0heKXv9z/it+lf3cvlb2tw74/0mHfu0XEL2vo42rg7Z3U/RB4p6RjKK4cqr0AQ3HuR0gqP/cPoZiuqlXHc7waOL3DmHaNiMo+5wDnSHoPxTTU/VX2+SLwe4qpu8o+9on0aaaIeBV4GLgUeDwi/gD8EvgE8FREvJjavRARF0bEwcBHgBsq9zQ60fFxXFPjmKqdh47npLPHquxLaT/vjIi9KYL9zXtMEfG9iDiR4vclgGtT+cqIOIfixflaYL6kPbo6kIp7W98CLqH4NNoQ4PHy8YBhksrrlXOymuLKYWjpfOwdEUfXMMYByeHQd5YCa4FrJO0haVdJ7011c4CPSzpM0p4U0zK3pauM31BcCbxPxcdOP08xH1urdRT3MurxY4p3qR+UtHP6+TNJR0XEGxT3SK6QtLukMcDkyoYR0U7xQnuupEHpnWv5ReJG4HJJR8ObN/nOrrFf3wb+XtK7VRiVfuGJiNeA+cD3gKURsaqTfSyhCLBPp3GdBPwlxX2UnroRuLrSF0nNkiaW6u+keHG7kuJx/lPHHaSybwHTJR2Q9jNM0mmlZg9SvLA9mNYf6LCOpLMlDU+rGyleUN/oou//Jz2OR1PcY6rcwO5uTN2ZCVwgaZyKm9vDJL2jSru9gFcoPiAxDPhUaSxHSjpZ0mCK+2y/r4xF0rmSmtN525Q26WqcUNwrCIo3OEi6gOLKoewA4G/Tc+Nsintjd0bEWuCnwFck7Z3G9HZJf1HrCRloHA59JL2I/iXFvOkqoI1iLh9gFsW0xyLgGYon/sfSdpuBj1K8ED5P8UJW05d4ki8Bn0+Xvn/fw76/DJxKMQWyhmKa51r+I6QuoZjqeQH4DvDPHXZxIcUv+W+Boyne4Vb2/YO0r7lpGuFx4PQa+/V9ijn771HcRP4hxc3bitkUN1Y7m1IiveM+Ix3zReAG4LyIeKKWPnTiOoobyT+V9DLFjdzjS8d8nSJQT0l978xnKG6WL07n5mcU96MqHqR4MV3UyToUN+uXSHol9enSiHimi2M+mI55L/DliPhpLWPqTkQspQib6RQ3dR/krVfLFV+g+MDDZorpt9tLdYOBaygepxcoXrg/m+rGA8vSOK8DJqU3CF31aTnwFYopunUUz5VfdGi2BBidjnk1cFZE/DbVnUfxIYblFME7n+JezHZJb51eM9t6ks6nuMF4YoP7cQjwBPC2iHipkX3Z1kkaSfHGZOdO7ovZDs5XDrZdSPcQPgHMdTCY1c/fbLQBL92IXEfxyZLsi1ZmtvU8rWRmZhlPK5mZWWbATisNHTo0Ro4c2ehumJkNKA8//PCLEdHtd6cGbDiMHDmSlpaWRnfDzGxAkVTTXzTwtJKZmWUcDmZmlnE4mJlZxuFgZmYZh4OZmWUcDmZmlnE4mJlZxuFgZmYZh4OZmWUG7DekzcwGipGX/aTX9vXsNe/rtX11ZYcMh4H4QJmZ9SdPK5mZWcbhYGZmGYeDmZllug0HSbMkrZf0eKnsHyU9IekxST+QNKRUd7mkVklPSjqtVD4+lbVKuqxUfpikJZJWSrpN0i69OUAzM9t6tVw5fIf8/+VdCBwTEe8EfgNcDiBpDDAJODptc4OkQZIGAd8ATgfGAOektgDXAtMjYjSwEZhS14jMzKxu3YZDRCwCNnQo+2lEbEmri4HhaXkiMDciXo+IZ4BW4Lj00xoRT0fEH4C5wERJAk4G5qftZwNn1jkmMzOrU2/cc/gQcFdaHgasLtW1pbLOyvcHNpWCplJelaSpkloktbS3t/dC183MrJq6wkHS54AtwK2VoirNogflVUXEjIgYGxFjm5u7/S9Qzcysh3r8JThJk4H3A+MiovKC3gaMKDUbDqxJy9XKXwSGSGpKVw/l9mZm1iA9unKQNB74DHBGRLxaqloATJI0WNJhwGhgKfAQMDp9MmkXipvWC1Ko3A+clbafDNzRs6GYmVlvqeWjrHOAXwFHSmqTNAX4J2AvYKGkRyXdCBARy4B5wHLgbuDiiHgjXRVcAtwDrADmpbZQhMwnJLVS3IOY2asjNDOzrdbttFJEnFOluNMX8Ii4Gri6SvmdwJ1Vyp+m+DSTmZltI/wNaTMzyzgczMws43AwM7OMw8HMzDIOBzMzyzgczMws43AwM7OMw8HMzDIOBzMzyzgczMws43AwM7OMw8HMzDIOBzMzyzgczMws43AwM7OMw8HMzDIOBzMzyzgczMws43AwM7OMw8HMzDIOBzMzyzgczMws43AwM7OMw8HMzDLdhoOkWZLWS3q8VLafpIWSVqZ/903lknS9pFZJj0k6trTN5NR+paTJpfJ3S/r3tM31ktTbgzQzs61Ty5XDd4DxHcouA+6NiNHAvWkd4HRgdPqZCnwTijABpgHHA8cB0yqBktpMLW3X8VhmZtbPug2HiFgEbOhQPBGYnZZnA2eWym+OwmJgiKSDgNOAhRGxISI2AguB8alu74j4VUQEcHNpX2Zm1iA9vedwYESsBUj/HpDKhwGrS+3aUllX5W1VyquSNFVSi6SW9vb2HnbdzMy609s3pKvdL4gelFcVETMiYmxEjG1ubu5hF83MrDs9DYd1aUqI9O/6VN4GjCi1Gw6s6aZ8eJVyMzNroJ6GwwKg8omjycAdpfLz0qeWTgA2p2mne4BTJe2bbkSfCtyT6l6WdEL6lNJ5pX2ZmVmDNHXXQNIc4CRgqKQ2ik8dXQPMkzQFWAWcnZrfCUwAWoFXgQsAImKDpKuAh1K7KyOicpP7IopPRO0G3JV+zMysgboNh4g4p5OqcVXaBnBxJ/uZBcyqUt4CHNNdP8zMrP/4G9JmZpZxOJiZWcbhYGZmGYeDmZllHA5mZpZxOJiZWcbhYGZmGYeDmZllHA5mZpZxOJiZWcbhYGZmGYeDmZllHA5mZpZxOJiZWcbhYGZmGYeDmZllHA5mZpZxOJiZWcbhYGZmGYeDmZllHA5mZpZxOJiZWcbhYGZmGYeDmZll6goHSR+XtEzS45LmSNpV0mGSlkhaKek2SbuktoPTemuqH1naz+Wp/ElJp9U3JDMzq1ePw0HSMOBvgbERcQwwCJgEXAtMj4jRwEZgStpkCrAxIkYB01M7JI1J2x0NjAdukDSop/0yM7P61Tut1ATsJqkJ2B1YC5wMzE/1s4Ez0/LEtE6qHydJqXxuRLweEc8ArcBxdfbLzMzq0ONwiIjngS8DqyhCYTPwMLApIrakZm3AsLQ8DFidtt2S2u9fLq+yjZmZNUA900r7UrzrPww4GNgDOL1K06hs0kldZ+XVjjlVUouklvb29q3vtJmZ1aSeaaVTgGcioj0i/gjcDvxXYEiaZgIYDqxJy23ACIBUvw+woVxeZZu3iIgZETE2IsY2NzfX0XUzM+tKPeGwCjhB0u7p3sE4YDlwP3BWajMZuCMtL0jrpPr7IiJS+aT0aabDgNHA0jr6ZWZmdWrqvkl1EbFE0nzg34AtwCPADOAnwFxJX0xlM9MmM4FbJLVSXDFMSvtZJmkeRbBsAS6OiDd62i8zM6tfj8MBICKmAdM6FD9NlU8bRcRrwNmd7Odq4Op6+mJmZr3H35A2M7OMw8HMzDIOBzMzyzgczMws43AwM7OMw8HMzDIOBzMzyzgczMws43AwM7OMw8HMzDIOBzMzyzgczMws43AwM7OMw8HMzDIOBzMzyzgczMws43AwM7OMw8HMzDIOBzMzyzgczMws43AwM7OMw8HMzDIOBzMzyzgczMwsU1c4SBoiab6kJyStkPQeSftJWihpZfp339RWkq6X1CrpMUnHlvYzObVfKWlyvYMyM7P61HvlcB1wd0S8A/jPwArgMuDeiBgN3JvWAU4HRqefqcA3ASTtB0wDjgeOA6ZVAsXMzBqjx+EgaW/gz4GZABHxh4jYBEwEZqdms4Ez0/JE4OYoLAaGSDoIOA1YGBEbImIjsBAY39N+mZlZ/eq5cjgcaAf+WdIjkr4taQ/gwIhYC5D+PSC1HwasLm3flso6KzczswapJxyagGOBb0bEu4Df8R9TSNWoSll0UZ7vQJoqqUVSS3t7+9b218zMalRPOLQBbRGxJK3PpwiLdWm6iPTv+lL7EaXthwNruijPRMSMiBgbEWObm5vr6LqZmXWlx+EQES8AqyUdmYrGAcuBBUDlE0eTgTvS8gLgvPSppROAzWna6R7gVEn7phvRp6YyMzNrkKY6t/8YcKukXYCngQsoAmeepCnAKuDs1PZOYALQCrya2hIRGyRdBTyU2l0ZERvq7JeZmdWhrnCIiEeBsVWqxlVpG8DFnexnFjCrnr6YmVnv8Tekzcws43AwM7OMw8HMzDIOBzMzyzgczMws43AwM7OMw8HMzDIOBzMzyzgczMws43AwM7OMw8HMzDIOBzMzyzgczMws43AwM7OMw8HMzDIOBzMzyzgczMws43AwM7OMw8HMzDIOBzMzyzgczMws43AwM7OMw8HMzDIOBzMzyzgczMwsU3c4SBok6RFJP07rh0laImmlpNsk7ZLKB6f11lQ/srSPy1P5k5JOq7dPZmZWn964crgUWFFavxaYHhGjgY3AlFQ+BdgYEaOA6akdksYAk4CjgfHADZIG9UK/zMysh+oKB0nDgfcB307rAk4G5qcms4Ez0/LEtE6qH5faTwTmRsTrEfEM0AocV0+/zMysPvVeOXwN+DTwp7S+P7ApIrak9TZgWFoeBqwGSPWbU/s3y6ts8xaSpkpqkdTS3t5eZ9fNzKwzPQ4HSe8H1kfEw+XiKk2jm7qutnlrYcSMiBgbEWObm5u3qr9mZla7pjq2fS9whqQJwK7A3hRXEkMkNaWrg+HAmtS+DRgBtElqAvYBNpTKK8rbmJlZA/T4yiEiLo+I4RExkuKG8n0R8TfA/cBZqdlk4I60vCCtk+rvi4hI5ZPSp5kOA0YDS3vaLzMzq189Vw6d+QwwV9IXgUeAmal8JnCLpFaKK4ZJABGxTNI8YDmwBbg4It7og36ZmVmNeiUcIuIB4IG0/DRVPm0UEa8BZ3ey/dXA1b3RFzMzq5+/IW1mZhmHg5mZZRwOZmaWcTiYmVnG4WBmZhmHg5mZZRwOZmaWcTiYmVnG4WBmZhmHg5mZZRwOZmaWcTiYmVnG4WBmZhmHg5mZZRwOZmaWcTiYmVnG4WBmZhmHg5mZZRwOZmaWcTiYmVnG4WBmZhmHg5mZZRwOZmaWcTiYmVmmx+EgaYSk+yWtkLRM0qWpfD9JCyWtTP/um8ol6XpJrZIek3RsaV+TU/uVkibXPywzM6tHPVcOW4BPRsRRwAnAxZLGAJcB90bEaODetA5wOjA6/UwFvglFmADTgOOB44BplUAxM7PG6HE4RMTaiPi3tPwysAIYBkwEZqdms4Ez0/JE4OYoLAaGSDoIOA1YGBEbImIjsBAY39N+mZlZ/XrlnoOkkcC7gCXAgRGxFooAAQ5IzYYBq0ubtaWyzsqrHWeqpBZJLe3t7b3RdTMzq6LucJC0J/AvwN9FxEtdNa1SFl2U54URMyJibESMbW5u3vrOmplZTeoKB0k7UwTDrRFxeypel6aLSP+uT+VtwIjS5sOBNV2Um5lZg9TzaSUBM4EVEfHVUtUCoPKJo8nAHaXy89Knlk4ANqdpp3uAUyXtm25En5rKzMysQZrq2Pa9wAeBf5f0aCr7LHANME/SFGAVcHaquxOYALQCrwIXAETEBklXAQ+ldldGxIY6+mVmZnXqcThExM+pfr8AYFyV9gFc3Mm+ZgGzetoXMzPrXf6GtJmZZRwOZmaWcTiYmVnG4WBmZhmHg5mZZRwOZmaWcTiYmVnG4WBmZhmHg5mZZRwOZmaWcTiYmVnG4WBmZhmHg5mZZRwOZmaWcTiYmVnG4WBmZhmHg5mZZRwOZmaWcTiYmVnG4WBmZhmHg5mZZRwOZmaWcTiYmVnG4WBmZpltJhwkjZf0pKRWSZc1uj9mZjuybSIcJA0CvgGcDowBzpE0prG9MjPbcW0T4QAcB7RGxNMR8QdgLjCxwX0yM9thNTW6A8kwYHVpvQ04vmMjSVOBqWn1FUlP9vB4Q4EXe7jtW/t0bW/spV/02pgHEI95+7ejjRddW/eYD62l0bYSDqpSFllBxAxgRt0Hk1oiYmy9+xlIPOYdw4425h1tvNB/Y95WppXagBGl9eHAmgb1xcxsh7ethMNDwGhJh0naBZgELGhwn8zMdljbxLRSRGyRdAlwDzAImBURy/rwkHVPTQ1AHvOOYUcb8442XuinMSsim9o3M7Md3LYyrWRmZtsQh4OZmWW263Do7k9ySBos6bZUv0TSyP7vZe+pYbyfkLRc0mOS7pVU0+edt2W1/tkVSWdJCkkD/mOPtYxZ0l+nx3qZpO/1dx97Ww3P7UMk3S/pkfT8ntCIfvYWSbMkrZf0eCf1knR9Oh+PSTq21zsREdvlD8WN7aeAw4FdgF8DYzq0+ShwY1qeBNzW6H738Xj/O7B7Wr5oII+31jGndnsBi4DFwNhG97sfHufRwCPAvmn9gEb3ux/GPAO4KC2PAZ5tdL/rHPOfA8cCj3dSPwG4i+I7YicAS3q7D9vzlUMtf5JjIjA7Lc8Hxkmq9oW8gaDb8UbE/RHxalpdTPF9koGs1j+7chXwD8Br/dm5PlLLmC8EvhERGwEiYn0/97G31TLmAPZOy/swwL8nFRGLgA1dNJkI3ByFxcAQSQf1Zh+253Co9ic5hnXWJiK2AJuB/fuld72vlvGWTaF45zGQdTtmSe8CRkTEj/uzY32olsf5COAISb+QtFjS+H7rXd+oZcxXAOdKagPuBD7WP11rmK39fd9q28T3HPpILX+So6Y/2zFA1DwWSecCY4G/6NMe9b0uxyxpJ2A6cH5/dagf1PI4N1FMLZ1EcXX4r5KOiYhNfdy3vlLLmM8BvhMRX5H0HuCWNOY/9X33GqLPX7u25yuHWv4kx5ttJDVRXI52dSm3LavpT5BIOgX4HHBGRLzeT33rK92NeS/gGOABSc9SzM0uGOA3pWt9Xt8REX+MiGeAJynCYqCqZcxTgHkAEfErYFeKP8q3verzPzm0PYdDLX+SYwEwOS2fBdwX6W7PANTteNMUy00UwTDQ56GhmzFHxOaIGBoRIyNiJMV9ljMioqUx3e0VtTyvf0jx4QMkDaWYZnq6X3vZu2oZ8ypgHICkoyjCob1fe9m/FgDnpU8tnQBsjoi1vXmA7XZaKTr5kxySrgRaImIBMJPi8rOV4ophUuN6XJ8ax/uPwJ7A99N991URcUbDOl2nGse8XalxzPcAp0paDrwBfCoiftu4XtenxjF/EviWpI9TTK+cP4Df6CFpDsW04NB0H2UasDNARNxIcV9lAtAKvApc0Ot9GMDnz8zM+sj2PK1kZmY95HAwM7OMw8HMzDIOBzMzyzgczMws43AwM7OMw8HMzDL/Hx4w5L1jMYl/AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x213c700eb00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(imdb_train['sentiment'], bins = \"auto\")\n",
    "plt.title(\"count frequency of reviews per class label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a large right tail to the length of review, meaning some reviewers are very verbose (over 2000 unique words in one review!), while others are very curt. The minimum length is four unique words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Primary plot!Primary direction!Poor interpretation.']\n"
     ]
    }
   ],
   "source": [
    "print(imdb_train[imdb_train[\"review\"].apply(lambda x: len(str.split(x)) == imdb_train['word_counts'].min())][\"review\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Back in the mid/late 80s, an OAV anime by title of \"Bubblegum Crisis\" (which I think is a military slang term for when technical equipment goes haywire) made its debut on video, taking inspiration from \"Blade Runner\", \"The Terminator\" and maybe even \"Robocop\", with a little dash of Batman/Bruce Wayne - Iron Man/Tony Stark and Charlie\\'s Angel\\'s girl power thrown in for good measure. 8 episodes long, the overall story was that in 21st century Tokyo, Japan, year 2032-2033, living machines called Boomers were doing manual labor and sometimes cause problems. A special, SWAT like branch of law enforcers, the Advanced Police (AD Police for short) were formed to handle the boomers, but were mostly ineffective, prompting millionaire scientist Sylia Stingray, the daughter of the scientist who made the boomers, to create four powered combat armor (hard suits) to be worn by women to fight the boomers and fight the evil corporation that produced the boomers, GENOM. That group becomes known as the Knight Sabers, and in addition to ring leader Sylia, her rag-tag band of rebel women included Priss Asagiri, a struggling rock and roll gal with a passion for motorcycles and a disdain for cops, Linna Yamazaki, an aerobics instructor with an eye for money and a tendency to blow through boyfriends, and Nene Romanova, a young officer of the ADP and expert computer hacker (the first in a long line). GENOM, meanwhile, is represented by Quincy, a tall, gaunt old guy who happens to own the company, his younger assistant Brian J. Mason (killed in episode 3) and an annoying boomer man named Largo. Other characters included Leon McNichol and Daley Wong, two AD Police detectives (Leon appeared in a spin-off/prequel anime, \"AD Police Files\" which I heard was very dark), their balding, overweight boss Chief Todo, Sylia\\'s younger brother Mackey, and a funny little mechanic known as Dr. Raven, who apparently helps Sylia with maintaining the suits. Aside from the overall Knight Sabers & AD Police VS GENOM storyline, there was also another storyline involving a friend of Linna\\'s who was apparently a daughter in a big crime family, the annoying Largo trying to usurp GENOM, and various Priss-wants-revenge-for-a-minor-character story. Oh and did I mention that there were hints that Sylia herself might have been a boomer?<br /><br />Well, it was a great watch, full of chaos and mayhem and even some very nice pop songs, but it was not without its flaws, some of which, unfortunately, were due to the fact that the series was discontinued after episode 8 when it was originally planned for 13 episodes in all. So some of the storylines, like Largo\\'s scheme (or schemes), the family of Linna\\'s ill-fated friend, and Sylia\\'s origins, were never resolved. Another problem with the series was that at the time Priss was the most popular character, so a good portion of the series focused on her, and unfortunately, most of the Priss oriented episodes basically focused on Priss self-righteously seeking justice/revenge for some secondary character who had never appeared before but happened to be a friend of hers, yet she rarely went out of her way for her the Knight Sabers, who were always bailing her out of trouble and for some reason cared a great deal about her well-being (just to be fair though, she did go to rescue Linna in episode 7, and her boyfriend got killed by a boomer and the ADP acted wrongly in the investigation). This meant we didn\\'t really get to focus on the more interesting back story of Sylia, or even the day-to-day antics of Nene and Linna. Linna had two episodes oriented around her, which pertained to her friend with the mafia family, while Nene managed to snag the last episode for herself, which showed her eternal good cheer was genuinely good spirits and not ditziness. Nene also got to put her computer skills to good use quite a bit, or she sometimes just acted like a lovable goof, which put her screen time and character development a few notches above poor Linna, who was often thrust into the background with only her greed and her tendency to eat up boyfriends to get her any attention. Don\\'t get me wrong, I like it and I love the overall concept of it all, but it did irk me a little bit. Also this is one of those runner-ups for \"worst English voice dubbing of all time\" features, meaning you\\'d better stick to the Japanese. Some of the voices were okay (some really did match their characters personas) but others were just flat and passionless or, in the case of Priss, really overacted.<br /><br />Well, Tokyo 2040 comes along and pretty much tosses all that out the window. Set a few years ahead, the story here is that after earthquakes shattered Tokyo, GENOM\\'s boomers rebuilt the city into a big old paradise, except the boomers still have a tendency to fly off the handle, which prompts the AD Police to be formed followed by the Knight Sabers being formed. So the overall story is the same, but the backstories of the characters and the look and attitudes of the characters have changed a lot.<br /><br />1) Originally Sylia had short purplish black hair and brown eyes, was usually dressed like a stern, proper business woman and was distant from others. 2040 Sylia has more of a super-model look to her, dressing more provocatively and possessing white hair and blue eyes that seem to change color depending on the light (runs the gamut from blue to purple to silver and eyes occasionally looking purple or gray), and also 2040 Sylia is more of an emotionally unstable woman who flies off the handle when she\\'s not in public, and possibly keeps even more secrets than before. Sylia also doesn\\'t take as much risk on the battlefield, as she is more of a stay-in-the-mobile-pit type here, but she does do battle when she has to.<br /><br />2) Originally Priss was a short woman with an Afro and a really bad temper, always picking fights with people who offended her, always biting off more than she could chew, etc. 2040 Priss, however, has gone the way of the Clint Eastwood loner - very cold, very stoic and emotionally distant (more like the original Sylia you might say), so she\\'s not really attached to anyone. Also her hair is more stingry and cat-like (a big improvement) and she is clad in leather like Trinity from \"The Matrix\" (although much less annoying than before, unfortunately, the writers screw her in the end when revealing her reasons for hating the ADP).<br /><br />3) Originally Linna had this big black hair going for her, but now her hair is shorter, browner, and, well, more 90s like. 2040 Linna is also an office lady who has bad luck with being sexually harrassed. As if to apologize for the way she was treated by the OAV writers, the 2040 writers actually dedicated the first 6 episodes to Linna, writing her as a country girl new to the city but determined to meet the Knight Sabers and win a spot with them, which she eventually does.<br /><br />4) Originally a short red haired girl who was often the victim of ridicule and ate a lot of candy, Nene is now a short blonde haired girl who likes to tease and take pot shots at ADP detective Leon McNichol (revenge for him toying with her in the OAV?) and other characters, even her surrogate big sister Linna and Mackey, Sylia\\'s \"brother\", whom she becomes infatuated with. Cockey and arrogant, she still eats a lot of candy and is a master hacker, but she is eventually deflated and grows beyond her comic relief status.<br /><br />5) Nigel Kirkland is a new character, a tall, stoic, ruggedly handsome man with long black hair (he looks like Adrian Paul from TV\\'s Highlander), he replaces Dr. Raven from the old series and now serves as the man who gives maitenance to Sylia\\'s hard suits. Nigel is also Sylia\\'s lover, but you wouldn\\'t know it by his demeanor. He\\'s kind of the father/big brother/mentor figure to Mackey.<br /><br />6) Leon and Daley are back, but of course differently. The original Leon was a tall pretty boy built like a baseball player with slicked back brown hair, blue eyes, a black leather jacket, tight blue jeans, and always carrying a revolver that could magically pack more whallop than a howitzer if necessary; while he wasn\\'t really a bad guy deep down, he was kind of a jerk, but he served mostly as comic relief, as he tried to pursue Priss romantically (exactly what he saw in her is a mystery) but occasionally he and Daley served as information guides to important plot points. Also the original Daley was a fairly muscular red head who dressed in pink/purple suits as he was a flamboyantly homosexual character who was always hitting on Leon when not providing important information. In 2040, Leon is no longer a pretty boy but more your typical rugged tough guy type, with spiked black hair, brown eyes, tall and sporting big muscles, a brown leather jacket and blue dockers (he actually looks like Arnold Schwarzenegger a little bit, or maybe a pumped up Colin Farrell, or Hugh Jackman), and he still carries a revolver, a BIG one, but it\\'s not as powerful as before. Although 2040 Leon still has a bit of an attitude probelm (especially in approaching Priss), he\\'s not nearly as much of a jerk as he was in the old series, but he does have a bad temper and he is easily annoyed by Nene and Daley (also he drinks way too much coffee). Oh, and Leon is still after Priss, but he has a lot more luck this time around. Daley, meanwhile, is now a taller (but not as tall as Leon) more pretty boyish looking guy with red rimmed glasses, a white suit, green eyes, and light brown hair, and he carries a big machine gun (he actually looks like James Marsden from the X-Men films); Daley is a lot smarter and more assertive in 2040 than the OAV and, although it\\'s not completely clear, his homosexual tendencies have been almost totally disappeared, save a moment of what appears to be jealousy when he hears about Leon inquiring about Priss\\'s e-mail.<br /><br />7) Brian J. Mason (what does that \"J\" stand for?) is back, and so is Quincy, but Mason is much more the main villain here, with Quincy as co-villain, as he is no longer a towering figure of terror but a vegetable with a bunch of batteries and wires plugged into him. Mason now sports slicked back brown hair instead of black hair as he did in the OAV (he actually looks like OAV Leon in a suit) and he is very much from the Alan Rickman school of villains.<br /><br />8) Though a pervert in the first series, Mackey is no longer a pervert in 2040. Of course, there are lots of things different about Mackey in 2040, but they won\\'t be revealed here.<br /><br />9) Sylia now has a companion, an Alfred-the-butler type named Henderson, who worries about her and the gang.<br /><br />10) In the original series, boomers were like the Replicants in \"Blade Runner\", armed with their own thoughts and feelings and ambitions, but in 2040, they\\'re more of the dumb-monsters-on-the-rampage type. Most of the time they\\'re just big robots who do whatever they\\'re programmed to do (heavy labor, combat, clean up, etc) and they have this tendency to \"go rogue\", which means try to evolve and become a monster in the process.<br /><br />What does stay the same is the theme of humanity VS technology (do machines have souls?). Sadly, this series, though well animated and well written, only runs 26 episodes, so it moves by faster than one might like, especially those of us who are used to more than one season of our most beloved characters, and unfortunately it still ends on a cliff hanger with unresolved storyline bits (which I will not discuss here. What saves this show and makes it what it is, however, is the characters, a colorful cast of screwballs they are, ranging from stoic loners, psycho women, genocidal mad men, rough neck cops, sardonic intellectuals, wise old sages, and loveable innocents, much more diverse than before and with a lot more to play off of, they\\'re enough to make you wish this show had gone longer.<br /><br />It\\'s not great, but it\\'s a good watch. Also the English dub (by ADV) is quite good, though not without a few flat spots, but certainly better than the dub on the original.']"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(imdb_train[imdb_train[\"review\"].apply(lambda x: len(str.split(x)) == imdb_train['word_counts'].max())][\"review\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasEmbeddingTextClassifier::create_pipeline ==> start\n",
      "Word2VecVectorizer::load_embeddings ==> start\n",
      "Time taken: 0.09 mins\n",
      "Word2VecVectorizer::load_embeddings ==> end\n",
      "num_words=100002\n",
      ":: number of jobs for the pipeline : 8\n",
      "0\tnltk_preprocessor\n",
      "1\tvectorizer\n",
      "2\tlearner\n",
      "KerasEmbeddingTextClassifier::create_pipeline ==> end\n"
     ]
    }
   ],
   "source": [
    "from tatk.pipelines.text_classification.keras_embedding_text_classifier import KerasEmbeddingTextClassifier\n",
    "import datetime\n",
    "log_dir = pathlib.Path(resources_dir) / \"logs\"\n",
    "imdb_logs = str(log_dir / \"imdb_results\" / datetime.datetime.now().strftime(\"%Y_%m_%d\") / \"kernel_3\")\n",
    "\n",
    "# the generic word embedding model has 3M words. We load the top <max_features> words as input features to the neural network\n",
    "max_features=100000\n",
    "\n",
    "keras_text_classifier = KerasEmbeddingTextClassifier(embedding_file_path, \n",
    "                                                     input_col=\"review\", \n",
    "                                                     label_cols=\"sentiment\",\n",
    "                                                     model_type=\"CNN\",\n",
    "                                                     binary_format=True, \n",
    "                                                     limit=max_features,\n",
    "                                                     cuda_devices=\"0\",\n",
    "                                                     callbacks=True,\n",
    "                                                     log_dir=imdb_logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View and Modify Pipeline and Model Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using a slice of the full word embedding matrix (i.e., specified by the `limit` parameter and `max_features` value above) it'll be more efficient to not save the entire word-embedding matrix in our saved pipeline. This can be specified by modifying the `get_from_path` argument in the word2vec `vectorizer`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'binary_format': True,\n",
       " 'callbacks': True,\n",
       " 'class_type': 'single-label',\n",
       " 'cuda_devices': '0',\n",
       " 'embedding_file_path': 'C:\\\\Users\\\\alizaidi\\\\tatk\\\\resources\\\\models\\\\Word2Vec_Models/GoogleNews-vectors-negative300.bin',\n",
       " 'input_col': 'review',\n",
       " 'label_cols': ['sentiment'],\n",
       " 'limit': 100000,\n",
       " 'log_dir': 'C:\\\\Users\\\\alizaidi\\\\tatk\\\\resources\\\\logs\\\\imdb_results\\\\2018_06_12\\\\kernel_3',\n",
       " 'model_type': 'CNN',\n",
       " 'n_labels': None,\n",
       " 'regex': None,\n",
       " 'trainable_embedding': False}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_text_classifier.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 50,\n",
       " 'callbacks_list': ['tensorboard', 'checkpoint', 'early'],\n",
       " 'cuda_devices': '0',\n",
       " 'feature_cols': ['features'],\n",
       " 'input_padding_value': 100001,\n",
       " 'label_cols': ['sentiment'],\n",
       " 'log_path': 'C:\\\\Users\\\\alizaidi\\\\tatk\\\\resources\\\\logs\\\\imdb_results\\\\2018_06_12\\\\kernel_3',\n",
       " 'max_len': None,\n",
       " 'model__class_type': 'single-label',\n",
       " 'model__dropout_rate': 0.25,\n",
       " 'model__hidden_dims': 250,\n",
       " 'model__init_wordvecs': array([[ 1.12915039e-03, -8.96453857e-04,  3.18527222e-04, ...,\n",
       "         -1.56402588e-03, -1.23023987e-04, -8.63075256e-05],\n",
       "        [ 7.03125000e-02,  8.69140625e-02,  8.78906250e-02, ...,\n",
       "         -4.76074219e-02,  1.44653320e-02, -6.25000000e-02],\n",
       "        [-1.17797852e-02, -4.73632812e-02,  4.46777344e-02, ...,\n",
       "          7.12890625e-02, -3.49121094e-02,  2.41699219e-02],\n",
       "        ...,\n",
       "        [ 5.03540039e-03, -9.57031250e-02,  1.75781250e-01, ...,\n",
       "          1.84570312e-01, -2.13867188e-01,  2.63671875e-01],\n",
       "        [ 2.73441649e-02,  2.15365560e-02, -1.05816893e-01, ...,\n",
       "          1.79734019e-04, -4.25357523e-02, -5.75520077e-02],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00]]),\n",
       " 'model__kernel_size': [3],\n",
       " 'model__num_filters': 250,\n",
       " 'model__trainable_embedding': False,\n",
       " 'model__wordvecs_shape': (100002, 300),\n",
       " 'model_fn': <function tatk.estimators.keras_model_functions.keras_CNN_text_classifier_fn.keras_CNN_text_classifier_fn(with_embedding_layer, model_params, n_labels, num_features=None, num_sent_features=0)>,\n",
       " 'model_type': 'CNN',\n",
       " 'n_epochs': 15,\n",
       " 'n_labels': None,\n",
       " 'prediction_col': 'prediction',\n",
       " 'probabilities_col': 'probabilities',\n",
       " 'validation_split': 0.1}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_text_classifier.get_step_params_by_name(\"learner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_text_classifier.set_step_params_by_name(\"learner\", model__kernel_size=[3])\n",
    "keras_text_classifier.set_step_params_by_name(\"vectorizer\", get_from_path=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasEmbeddingTextClassifier::fit ==> start\n",
      "schema: col=review:TX:0 col=sentiment:I8:1 col=word_counts:I8:2 header+\n",
      "NltkPreprocessor::tatk_fit_transform ==> start\n",
      "NltkPreprocessor::tatk_fit_transform ==> end \t Time taken: 0.34 mins\n",
      "Word2VecVectorizer::tatk_fit_transform ==> start\n",
      "Word2VecVectorizer::tatk_fit_transform ==> end \t Time taken: 0.26 mins\n",
      "KerasEmbeddingTextClassifierLearner::tatk_fit ==> start\n",
      "WARNING:tensorflow:From C:\\Users\\alizaidi\\AppData\\local\\AmlWorkbench\\Python\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, None, 300)         30000600  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, None, 250)         225250    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 502       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 30,289,102\n",
      "Trainable params: 288,502\n",
      "Non-trainable params: 30,000,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204/450 [============>.................] - ETA: 10:01 - loss: 0.6779 - acc: 0.56 - ETA: 9:30 - loss: 0.6996 - acc: 0.5400 - ETA: 12:08 - loss: 0.6994 - acc: 0.52 - ETA: 11:15 - loss: 0.7047 - acc: 0.51 - ETA: 10:51 - loss: 0.7135 - acc: 0.52 - ETA: 13:49 - loss: 0.7181 - acc: 0.51 - ETA: 13:34 - loss: 0.7133 - acc: 0.52 - ETA: 13:03 - loss: 0.7110 - acc: 0.53 - ETA: 13:04 - loss: 0.7072 - acc: 0.53 - ETA: 12:38 - loss: 0.7086 - acc: 0.52 - ETA: 12:31 - loss: 0.7054 - acc: 0.52 - ETA: 12:19 - loss: 0.7019 - acc: 0.54 - ETA: 12:02 - loss: 0.6987 - acc: 0.54 - ETA: 12:00 - loss: 0.6959 - acc: 0.55 - ETA: 11:45 - loss: 0.6945 - acc: 0.55 - ETA: 11:38 - loss: 0.6933 - acc: 0.55 - ETA: 11:37 - loss: 0.6916 - acc: 0.55 - ETA: 11:30 - loss: 0.6915 - acc: 0.54 - ETA: 11:10 - loss: 0.6906 - acc: 0.54 - ETA: 11:07 - loss: 0.6901 - acc: 0.55 - ETA: 11:12 - loss: 0.6912 - acc: 0.54 - ETA: 10:57 - loss: 0.6918 - acc: 0.54 - ETA: 10:49 - loss: 0.6909 - acc: 0.54 - ETA: 10:54 - loss: 0.6893 - acc: 0.55 - ETA: 10:47 - loss: 0.6881 - acc: 0.55 - ETA: 10:41 - loss: 0.6868 - acc: 0.55 - ETA: 10:42 - loss: 0.6862 - acc: 0.55 - ETA: 10:42 - loss: 0.6869 - acc: 0.55 - ETA: 10:45 - loss: 0.6833 - acc: 0.55 - ETA: 10:45 - loss: 0.6812 - acc: 0.56 - ETA: 10:41 - loss: 0.6785 - acc: 0.56 - ETA: 10:39 - loss: 0.6768 - acc: 0.56 - ETA: 10:36 - loss: 0.6741 - acc: 0.57 - ETA: 10:36 - loss: 0.6714 - acc: 0.58 - ETA: 10:30 - loss: 0.6699 - acc: 0.58 - ETA: 10:27 - loss: 0.6676 - acc: 0.58 - ETA: 10:25 - loss: 0.6646 - acc: 0.59 - ETA: 10:21 - loss: 0.6627 - acc: 0.59 - ETA: 10:17 - loss: 0.6617 - acc: 0.59 - ETA: 10:17 - loss: 0.6591 - acc: 0.59 - ETA: 10:14 - loss: 0.6582 - acc: 0.59 - ETA: 10:11 - loss: 0.6547 - acc: 0.60 - ETA: 10:07 - loss: 0.6535 - acc: 0.60 - ETA: 10:05 - loss: 0.6507 - acc: 0.60 - ETA: 10:07 - loss: 0.6507 - acc: 0.60 - ETA: 10:04 - loss: 0.6482 - acc: 0.60 - ETA: 10:04 - loss: 0.6466 - acc: 0.61 - ETA: 10:02 - loss: 0.6433 - acc: 0.61 - ETA: 10:04 - loss: 0.6395 - acc: 0.61 - ETA: 10:03 - loss: 0.6384 - acc: 0.62 - ETA: 10:04 - loss: 0.6370 - acc: 0.62 - ETA: 10:04 - loss: 0.6363 - acc: 0.62 - ETA: 10:01 - loss: 0.6342 - acc: 0.62 - ETA: 10:01 - loss: 0.6307 - acc: 0.62 - ETA: 9:57 - loss: 0.6302 - acc: 0.6291 - ETA: 9:56 - loss: 0.6277 - acc: 0.630 - ETA: 9:57 - loss: 0.6263 - acc: 0.633 - ETA: 9:57 - loss: 0.6241 - acc: 0.634 - ETA: 9:54 - loss: 0.6205 - acc: 0.637 - ETA: 9:52 - loss: 0.6202 - acc: 0.637 - ETA: 9:51 - loss: 0.6194 - acc: 0.639 - ETA: 9:47 - loss: 0.6158 - acc: 0.643 - ETA: 9:46 - loss: 0.6144 - acc: 0.644 - ETA: 9:44 - loss: 0.6129 - acc: 0.645 - ETA: 9:42 - loss: 0.6093 - acc: 0.648 - ETA: 9:40 - loss: 0.6072 - acc: 0.651 - ETA: 9:37 - loss: 0.6046 - acc: 0.652 - ETA: 9:35 - loss: 0.6013 - acc: 0.655 - ETA: 9:34 - loss: 0.5986 - acc: 0.657 - ETA: 9:40 - loss: 0.5977 - acc: 0.659 - ETA: 9:38 - loss: 0.5949 - acc: 0.662 - ETA: 9:38 - loss: 0.5926 - acc: 0.663 - ETA: 9:33 - loss: 0.5907 - acc: 0.664 - ETA: 9:31 - loss: 0.5882 - acc: 0.666 - ETA: 9:39 - loss: 0.5869 - acc: 0.668 - ETA: 9:38 - loss: 0.5837 - acc: 0.671 - ETA: 9:35 - loss: 0.5835 - acc: 0.671 - ETA: 9:32 - loss: 0.5823 - acc: 0.673 - ETA: 9:31 - loss: 0.5805 - acc: 0.674 - ETA: 9:30 - loss: 0.5780 - acc: 0.676 - ETA: 9:29 - loss: 0.5755 - acc: 0.678 - ETA: 9:26 - loss: 0.5740 - acc: 0.679 - ETA: 9:24 - loss: 0.5715 - acc: 0.682 - ETA: 9:24 - loss: 0.5698 - acc: 0.683 - ETA: 9:21 - loss: 0.5669 - acc: 0.686 - ETA: 9:18 - loss: 0.5643 - acc: 0.687 - ETA: 9:16 - loss: 0.5645 - acc: 0.688 - ETA: 9:16 - loss: 0.5630 - acc: 0.689 - ETA: 9:14 - loss: 0.5607 - acc: 0.691 - ETA: 9:13 - loss: 0.5595 - acc: 0.692 - ETA: 9:12 - loss: 0.5575 - acc: 0.693 - ETA: 9:08 - loss: 0.5558 - acc: 0.694 - ETA: 9:06 - loss: 0.5563 - acc: 0.694 - ETA: 9:05 - loss: 0.5545 - acc: 0.696 - ETA: 9:04 - loss: 0.5526 - acc: 0.698 - ETA: 9:02 - loss: 0.5528 - acc: 0.698 - ETA: 9:03 - loss: 0.5507 - acc: 0.700 - ETA: 8:59 - loss: 0.5498 - acc: 0.700 - ETA: 8:58 - loss: 0.5490 - acc: 0.701 - ETA: 8:56 - loss: 0.5469 - acc: 0.703 - ETA: 8:52 - loss: 0.5452 - acc: 0.704 - ETA: 8:52 - loss: 0.5431 - acc: 0.706 - ETA: 8:50 - loss: 0.5423 - acc: 0.706 - ETA: 8:48 - loss: 0.5412 - acc: 0.707 - ETA: 8:46 - loss: 0.5406 - acc: 0.707 - ETA: 8:44 - loss: 0.5386 - acc: 0.709 - ETA: 8:40 - loss: 0.5387 - acc: 0.709 - ETA: 8:37 - loss: 0.5380 - acc: 0.710 - ETA: 8:35 - loss: 0.5367 - acc: 0.711 - ETA: 8:33 - loss: 0.5348 - acc: 0.712 - ETA: 8:32 - loss: 0.5337 - acc: 0.713 - ETA: 8:30 - loss: 0.5328 - acc: 0.714 - ETA: 8:27 - loss: 0.5304 - acc: 0.716 - ETA: 8:25 - loss: 0.5293 - acc: 0.717 - ETA: 8:23 - loss: 0.5277 - acc: 0.718 - ETA: 8:22 - loss: 0.5263 - acc: 0.718 - ETA: 8:20 - loss: 0.5253 - acc: 0.719 - ETA: 8:19 - loss: 0.5231 - acc: 0.721 - ETA: 8:18 - loss: 0.5222 - acc: 0.721 - ETA: 8:17 - loss: 0.5206 - acc: 0.722 - ETA: 8:15 - loss: 0.5206 - acc: 0.722 - ETA: 8:12 - loss: 0.5197 - acc: 0.723 - ETA: 8:11 - loss: 0.5187 - acc: 0.724 - ETA: 8:10 - loss: 0.5178 - acc: 0.725 - ETA: 8:08 - loss: 0.5170 - acc: 0.726 - ETA: 8:08 - loss: 0.5164 - acc: 0.726 - ETA: 8:06 - loss: 0.5150 - acc: 0.727 - ETA: 8:03 - loss: 0.5143 - acc: 0.728 - ETA: 8:01 - loss: 0.5142 - acc: 0.728 - ETA: 7:58 - loss: 0.5137 - acc: 0.728 - ETA: 7:58 - loss: 0.5141 - acc: 0.728 - ETA: 7:55 - loss: 0.5125 - acc: 0.729 - ETA: 7:53 - loss: 0.5127 - acc: 0.729 - ETA: 7:51 - loss: 0.5115 - acc: 0.730 - ETA: 7:50 - loss: 0.5105 - acc: 0.731 - ETA: 7:48 - loss: 0.5105 - acc: 0.731 - ETA: 7:47 - loss: 0.5092 - acc: 0.732 - ETA: 7:45 - loss: 0.5067 - acc: 0.733 - ETA: 7:43 - loss: 0.5065 - acc: 0.734 - ETA: 7:42 - loss: 0.5054 - acc: 0.734 - ETA: 7:41 - loss: 0.5041 - acc: 0.735 - ETA: 7:40 - loss: 0.5037 - acc: 0.736 - ETA: 7:39 - loss: 0.5032 - acc: 0.736 - ETA: 7:36 - loss: 0.5025 - acc: 0.736 - ETA: 7:35 - loss: 0.5025 - acc: 0.737 - ETA: 7:34 - loss: 0.5017 - acc: 0.738 - ETA: 7:33 - loss: 0.5012 - acc: 0.738 - ETA: 7:30 - loss: 0.4997 - acc: 0.739 - ETA: 7:28 - loss: 0.4991 - acc: 0.740 - ETA: 7:27 - loss: 0.4979 - acc: 0.741 - ETA: 7:25 - loss: 0.4973 - acc: 0.741 - ETA: 7:23 - loss: 0.4974 - acc: 0.741 - ETA: 7:21 - loss: 0.4965 - acc: 0.742 - ETA: 7:20 - loss: 0.4955 - acc: 0.742 - ETA: 7:18 - loss: 0.4943 - acc: 0.743 - ETA: 7:16 - loss: 0.4943 - acc: 0.743 - ETA: 7:16 - loss: 0.4944 - acc: 0.743 - ETA: 7:14 - loss: 0.4943 - acc: 0.743 - ETA: 7:13 - loss: 0.4936 - acc: 0.744 - ETA: 7:11 - loss: 0.4922 - acc: 0.745 - ETA: 7:10 - loss: 0.4908 - acc: 0.746 - ETA: 7:08 - loss: 0.4909 - acc: 0.746 - ETA: 7:06 - loss: 0.4895 - acc: 0.747 - ETA: 7:05 - loss: 0.4891 - acc: 0.747 - ETA: 7:04 - loss: 0.4884 - acc: 0.748 - ETA: 7:03 - loss: 0.4883 - acc: 0.749 - ETA: 7:01 - loss: 0.4870 - acc: 0.750 - ETA: 7:02 - loss: 0.4861 - acc: 0.750 - ETA: 7:04 - loss: 0.4857 - acc: 0.751 - ETA: 7:04 - loss: 0.4849 - acc: 0.751 - ETA: 7:06 - loss: 0.4841 - acc: 0.751 - ETA: 7:06 - loss: 0.4841 - acc: 0.751 - ETA: 7:05 - loss: 0.4832 - acc: 0.752 - ETA: 7:05 - loss: 0.4825 - acc: 0.752 - ETA: 7:05 - loss: 0.4811 - acc: 0.753 - ETA: 7:04 - loss: 0.4801 - acc: 0.754 - ETA: 7:02 - loss: 0.4798 - acc: 0.754 - ETA: 7:01 - loss: 0.4790 - acc: 0.755 - ETA: 6:59 - loss: 0.4786 - acc: 0.756 - ETA: 6:58 - loss: 0.4778 - acc: 0.756 - ETA: 6:57 - loss: 0.4765 - acc: 0.757 - ETA: 6:55 - loss: 0.4750 - acc: 0.758 - ETA: 6:53 - loss: 0.4744 - acc: 0.758 - ETA: 6:51 - loss: 0.4738 - acc: 0.758 - ETA: 6:50 - loss: 0.4728 - acc: 0.759 - ETA: 6:48 - loss: 0.4724 - acc: 0.759 - ETA: 6:47 - loss: 0.4716 - acc: 0.760 - ETA: 6:46 - loss: 0.4714 - acc: 0.760 - ETA: 6:45 - loss: 0.4704 - acc: 0.761 - ETA: 6:43 - loss: 0.4697 - acc: 0.761 - ETA: 6:42 - loss: 0.4692 - acc: 0.762 - ETA: 6:41 - loss: 0.4691 - acc: 0.762 - ETA: 6:39 - loss: 0.4684 - acc: 0.762 - ETA: 6:39 - loss: 0.4673 - acc: 0.763 - ETA: 6:37 - loss: 0.4666 - acc: 0.763 - ETA: 6:35 - loss: 0.4655 - acc: 0.764 - ETA: 6:33 - loss: 0.4649 - acc: 0.764 - ETA: 6:32 - loss: 0.4645 - acc: 0.764 - ETA: 6:30 - loss: 0.4636 - acc: 0.764 - ETA: 6:29 - loss: 0.4628 - acc: 0.765 - ETA: 6:27 - loss: 0.4631 - acc: 0.765 - ETA: 6:25 - loss: 0.4627 - acc: 0.765 - ETA: 6:24 - loss: 0.4618 - acc: 0.766 - ETA: 6:22 - loss: 0.4614 - acc: 0.7664"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "408/450 [==========================>...] - ETA: 6:21 - loss: 0.4608 - acc: 0.766 - ETA: 6:19 - loss: 0.4603 - acc: 0.767 - ETA: 6:18 - loss: 0.4607 - acc: 0.767 - ETA: 6:16 - loss: 0.4608 - acc: 0.767 - ETA: 6:14 - loss: 0.4600 - acc: 0.768 - ETA: 6:12 - loss: 0.4588 - acc: 0.768 - ETA: 6:10 - loss: 0.4577 - acc: 0.769 - ETA: 6:09 - loss: 0.4576 - acc: 0.769 - ETA: 6:07 - loss: 0.4567 - acc: 0.770 - ETA: 6:05 - loss: 0.4560 - acc: 0.770 - ETA: 6:04 - loss: 0.4552 - acc: 0.770 - ETA: 6:02 - loss: 0.4543 - acc: 0.771 - ETA: 6:00 - loss: 0.4532 - acc: 0.772 - ETA: 5:59 - loss: 0.4521 - acc: 0.772 - ETA: 5:57 - loss: 0.4521 - acc: 0.772 - ETA: 5:55 - loss: 0.4518 - acc: 0.772 - ETA: 5:53 - loss: 0.4516 - acc: 0.773 - ETA: 5:51 - loss: 0.4520 - acc: 0.773 - ETA: 5:50 - loss: 0.4513 - acc: 0.773 - ETA: 5:48 - loss: 0.4511 - acc: 0.773 - ETA: 5:47 - loss: 0.4516 - acc: 0.774 - ETA: 5:45 - loss: 0.4507 - acc: 0.774 - ETA: 5:44 - loss: 0.4510 - acc: 0.774 - ETA: 5:42 - loss: 0.4505 - acc: 0.774 - ETA: 5:41 - loss: 0.4496 - acc: 0.775 - ETA: 5:39 - loss: 0.4496 - acc: 0.775 - ETA: 5:38 - loss: 0.4497 - acc: 0.775 - ETA: 5:36 - loss: 0.4495 - acc: 0.775 - ETA: 5:34 - loss: 0.4485 - acc: 0.775 - ETA: 5:33 - loss: 0.4481 - acc: 0.776 - ETA: 5:31 - loss: 0.4479 - acc: 0.776 - ETA: 5:29 - loss: 0.4478 - acc: 0.776 - ETA: 5:28 - loss: 0.4472 - acc: 0.776 - ETA: 5:26 - loss: 0.4463 - acc: 0.776 - ETA: 5:24 - loss: 0.4459 - acc: 0.777 - ETA: 5:23 - loss: 0.4452 - acc: 0.777 - ETA: 5:21 - loss: 0.4450 - acc: 0.777 - ETA: 5:19 - loss: 0.4446 - acc: 0.778 - ETA: 5:18 - loss: 0.4440 - acc: 0.778 - ETA: 5:16 - loss: 0.4434 - acc: 0.778 - ETA: 5:14 - loss: 0.4433 - acc: 0.778 - ETA: 5:13 - loss: 0.4426 - acc: 0.779 - ETA: 5:11 - loss: 0.4428 - acc: 0.779 - ETA: 5:09 - loss: 0.4423 - acc: 0.779 - ETA: 5:08 - loss: 0.4425 - acc: 0.779 - ETA: 5:06 - loss: 0.4421 - acc: 0.779 - ETA: 5:05 - loss: 0.4414 - acc: 0.780 - ETA: 5:04 - loss: 0.4411 - acc: 0.780 - ETA: 5:02 - loss: 0.4405 - acc: 0.781 - ETA: 5:00 - loss: 0.4400 - acc: 0.781 - ETA: 5:00 - loss: 0.4396 - acc: 0.781 - ETA: 4:58 - loss: 0.4392 - acc: 0.781 - ETA: 4:56 - loss: 0.4389 - acc: 0.782 - ETA: 4:55 - loss: 0.4393 - acc: 0.782 - ETA: 4:53 - loss: 0.4387 - acc: 0.782 - ETA: 4:51 - loss: 0.4391 - acc: 0.782 - ETA: 4:49 - loss: 0.4389 - acc: 0.782 - ETA: 4:48 - loss: 0.4389 - acc: 0.782 - ETA: 4:46 - loss: 0.4389 - acc: 0.783 - ETA: 4:45 - loss: 0.4388 - acc: 0.783 - ETA: 4:43 - loss: 0.4386 - acc: 0.783 - ETA: 4:42 - loss: 0.4379 - acc: 0.783 - ETA: 4:40 - loss: 0.4372 - acc: 0.783 - ETA: 4:38 - loss: 0.4365 - acc: 0.784 - ETA: 4:37 - loss: 0.4364 - acc: 0.784 - ETA: 4:35 - loss: 0.4357 - acc: 0.785 - ETA: 4:33 - loss: 0.4356 - acc: 0.785 - ETA: 4:32 - loss: 0.4351 - acc: 0.785 - ETA: 4:30 - loss: 0.4346 - acc: 0.785 - ETA: 4:29 - loss: 0.4341 - acc: 0.786 - ETA: 4:27 - loss: 0.4331 - acc: 0.786 - ETA: 4:26 - loss: 0.4326 - acc: 0.787 - ETA: 4:24 - loss: 0.4323 - acc: 0.787 - ETA: 4:23 - loss: 0.4316 - acc: 0.787 - ETA: 4:22 - loss: 0.4308 - acc: 0.788 - ETA: 4:20 - loss: 0.4307 - acc: 0.788 - ETA: 4:19 - loss: 0.4301 - acc: 0.788 - ETA: 4:18 - loss: 0.4297 - acc: 0.788 - ETA: 4:16 - loss: 0.4294 - acc: 0.789 - ETA: 4:15 - loss: 0.4287 - acc: 0.789 - ETA: 4:14 - loss: 0.4287 - acc: 0.789 - ETA: 4:12 - loss: 0.4285 - acc: 0.790 - ETA: 4:11 - loss: 0.4283 - acc: 0.789 - ETA: 4:09 - loss: 0.4280 - acc: 0.790 - ETA: 4:08 - loss: 0.4281 - acc: 0.790 - ETA: 4:06 - loss: 0.4279 - acc: 0.790 - ETA: 4:05 - loss: 0.4273 - acc: 0.790 - ETA: 4:03 - loss: 0.4271 - acc: 0.790 - ETA: 4:02 - loss: 0.4267 - acc: 0.791 - ETA: 4:00 - loss: 0.4259 - acc: 0.791 - ETA: 3:59 - loss: 0.4262 - acc: 0.791 - ETA: 3:57 - loss: 0.4258 - acc: 0.791 - ETA: 3:56 - loss: 0.4250 - acc: 0.791 - ETA: 3:54 - loss: 0.4251 - acc: 0.792 - ETA: 3:52 - loss: 0.4245 - acc: 0.792 - ETA: 3:51 - loss: 0.4242 - acc: 0.792 - ETA: 3:50 - loss: 0.4239 - acc: 0.792 - ETA: 3:48 - loss: 0.4236 - acc: 0.792 - ETA: 3:47 - loss: 0.4230 - acc: 0.793 - ETA: 3:46 - loss: 0.4226 - acc: 0.793 - ETA: 3:44 - loss: 0.4221 - acc: 0.793 - ETA: 3:42 - loss: 0.4218 - acc: 0.794 - ETA: 3:41 - loss: 0.4213 - acc: 0.794 - ETA: 3:39 - loss: 0.4210 - acc: 0.794 - ETA: 3:38 - loss: 0.4205 - acc: 0.794 - ETA: 3:37 - loss: 0.4205 - acc: 0.794 - ETA: 3:35 - loss: 0.4201 - acc: 0.795 - ETA: 3:34 - loss: 0.4201 - acc: 0.795 - ETA: 3:32 - loss: 0.4198 - acc: 0.795 - ETA: 3:30 - loss: 0.4193 - acc: 0.795 - ETA: 3:29 - loss: 0.4190 - acc: 0.795 - ETA: 3:28 - loss: 0.4189 - acc: 0.795 - ETA: 3:26 - loss: 0.4186 - acc: 0.796 - ETA: 3:24 - loss: 0.4183 - acc: 0.796 - ETA: 3:23 - loss: 0.4179 - acc: 0.796 - ETA: 3:21 - loss: 0.4176 - acc: 0.796 - ETA: 3:20 - loss: 0.4177 - acc: 0.796 - ETA: 3:18 - loss: 0.4170 - acc: 0.797 - ETA: 3:16 - loss: 0.4167 - acc: 0.797 - ETA: 3:15 - loss: 0.4163 - acc: 0.797 - ETA: 3:13 - loss: 0.4161 - acc: 0.797 - ETA: 3:11 - loss: 0.4160 - acc: 0.797 - ETA: 3:10 - loss: 0.4153 - acc: 0.798 - ETA: 3:08 - loss: 0.4147 - acc: 0.798 - ETA: 3:07 - loss: 0.4146 - acc: 0.798 - ETA: 3:05 - loss: 0.4144 - acc: 0.798 - ETA: 3:04 - loss: 0.4144 - acc: 0.798 - ETA: 3:02 - loss: 0.4144 - acc: 0.798 - ETA: 3:01 - loss: 0.4138 - acc: 0.799 - ETA: 2:59 - loss: 0.4141 - acc: 0.799 - ETA: 2:57 - loss: 0.4136 - acc: 0.799 - ETA: 2:56 - loss: 0.4134 - acc: 0.799 - ETA: 2:54 - loss: 0.4128 - acc: 0.799 - ETA: 2:53 - loss: 0.4128 - acc: 0.799 - ETA: 2:51 - loss: 0.4122 - acc: 0.799 - ETA: 2:50 - loss: 0.4119 - acc: 0.800 - ETA: 2:48 - loss: 0.4114 - acc: 0.800 - ETA: 2:47 - loss: 0.4110 - acc: 0.800 - ETA: 2:45 - loss: 0.4110 - acc: 0.800 - ETA: 2:43 - loss: 0.4106 - acc: 0.801 - ETA: 2:41 - loss: 0.4099 - acc: 0.801 - ETA: 2:40 - loss: 0.4096 - acc: 0.801 - ETA: 2:38 - loss: 0.4093 - acc: 0.802 - ETA: 2:36 - loss: 0.4095 - acc: 0.802 - ETA: 2:35 - loss: 0.4093 - acc: 0.802 - ETA: 2:33 - loss: 0.4089 - acc: 0.802 - ETA: 2:31 - loss: 0.4084 - acc: 0.802 - ETA: 2:30 - loss: 0.4081 - acc: 0.802 - ETA: 2:28 - loss: 0.4083 - acc: 0.802 - ETA: 2:26 - loss: 0.4081 - acc: 0.802 - ETA: 2:25 - loss: 0.4077 - acc: 0.803 - ETA: 2:23 - loss: 0.4074 - acc: 0.803 - ETA: 2:21 - loss: 0.4071 - acc: 0.803 - ETA: 2:19 - loss: 0.4069 - acc: 0.803 - ETA: 2:18 - loss: 0.4068 - acc: 0.803 - ETA: 2:16 - loss: 0.4066 - acc: 0.803 - ETA: 2:14 - loss: 0.4064 - acc: 0.803 - ETA: 2:13 - loss: 0.4061 - acc: 0.803 - ETA: 2:11 - loss: 0.4064 - acc: 0.803 - ETA: 2:09 - loss: 0.4062 - acc: 0.803 - ETA: 2:08 - loss: 0.4061 - acc: 0.803 - ETA: 2:06 - loss: 0.4056 - acc: 0.804 - ETA: 2:04 - loss: 0.4054 - acc: 0.804 - ETA: 2:03 - loss: 0.4052 - acc: 0.804 - ETA: 2:01 - loss: 0.4046 - acc: 0.804 - ETA: 2:00 - loss: 0.4051 - acc: 0.804 - ETA: 1:58 - loss: 0.4055 - acc: 0.804 - ETA: 1:56 - loss: 0.4048 - acc: 0.804 - ETA: 1:55 - loss: 0.4048 - acc: 0.804 - ETA: 1:53 - loss: 0.4051 - acc: 0.804 - ETA: 1:51 - loss: 0.4051 - acc: 0.804 - ETA: 1:50 - loss: 0.4049 - acc: 0.804 - ETA: 1:48 - loss: 0.4053 - acc: 0.804 - ETA: 1:46 - loss: 0.4052 - acc: 0.804 - ETA: 1:45 - loss: 0.4051 - acc: 0.805 - ETA: 1:43 - loss: 0.4047 - acc: 0.805 - ETA: 1:42 - loss: 0.4045 - acc: 0.805 - ETA: 1:40 - loss: 0.4042 - acc: 0.805 - ETA: 1:39 - loss: 0.4043 - acc: 0.805 - ETA: 1:37 - loss: 0.4040 - acc: 0.806 - ETA: 1:35 - loss: 0.4042 - acc: 0.805 - ETA: 1:34 - loss: 0.4040 - acc: 0.806 - ETA: 1:32 - loss: 0.4042 - acc: 0.806 - ETA: 1:31 - loss: 0.4040 - acc: 0.806 - ETA: 1:29 - loss: 0.4039 - acc: 0.806 - ETA: 1:28 - loss: 0.4035 - acc: 0.806 - ETA: 1:26 - loss: 0.4030 - acc: 0.806 - ETA: 1:25 - loss: 0.4025 - acc: 0.807 - ETA: 1:23 - loss: 0.4023 - acc: 0.807 - ETA: 1:22 - loss: 0.4020 - acc: 0.807 - ETA: 1:20 - loss: 0.4020 - acc: 0.807 - ETA: 1:18 - loss: 0.4018 - acc: 0.807 - ETA: 1:17 - loss: 0.4014 - acc: 0.807 - ETA: 1:15 - loss: 0.4010 - acc: 0.807 - ETA: 1:14 - loss: 0.4011 - acc: 0.807 - ETA: 1:12 - loss: 0.4009 - acc: 0.808 - ETA: 1:11 - loss: 0.4011 - acc: 0.808 - ETA: 1:09 - loss: 0.4012 - acc: 0.808 - ETA: 1:08 - loss: 0.4009 - acc: 0.808 - ETA: 1:06 - loss: 0.4005 - acc: 0.808 - ETA: 1:05 - loss: 0.4005 - acc: 0.808 - ETA: 1:03 - loss: 0.4003 - acc: 0.808 - ETA: 1:02 - loss: 0.4001 - acc: 0.809 - ETA: 1:00 - loss: 0.4001 - acc: 0.8091"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "449/450 [============================>.] - ETA: 59s - loss: 0.4001 - acc: 0.809 - ETA: 57s - loss: 0.3998 - acc: 0.80 - ETA: 56s - loss: 0.3994 - acc: 0.80 - ETA: 54s - loss: 0.3990 - acc: 0.80 - ETA: 53s - loss: 0.3992 - acc: 0.80 - ETA: 51s - loss: 0.3992 - acc: 0.81 - ETA: 50s - loss: 0.3996 - acc: 0.80 - ETA: 48s - loss: 0.3993 - acc: 0.81 - ETA: 47s - loss: 0.3994 - acc: 0.80 - ETA: 45s - loss: 0.3989 - acc: 0.81 - ETA: 44s - loss: 0.3987 - acc: 0.81 - ETA: 42s - loss: 0.3985 - acc: 0.81 - ETA: 41s - loss: 0.3986 - acc: 0.81 - ETA: 39s - loss: 0.3985 - acc: 0.81 - ETA: 38s - loss: 0.3983 - acc: 0.81 - ETA: 37s - loss: 0.3979 - acc: 0.81 - ETA: 35s - loss: 0.3975 - acc: 0.81 - ETA: 34s - loss: 0.3972 - acc: 0.81 - ETA: 32s - loss: 0.3972 - acc: 0.81 - ETA: 31s - loss: 0.3972 - acc: 0.81 - ETA: 29s - loss: 0.3970 - acc: 0.81 - ETA: 28s - loss: 0.3967 - acc: 0.81 - ETA: 27s - loss: 0.3965 - acc: 0.81 - ETA: 25s - loss: 0.3962 - acc: 0.81 - ETA: 24s - loss: 0.3960 - acc: 0.81 - ETA: 22s - loss: 0.3959 - acc: 0.81 - ETA: 21s - loss: 0.3961 - acc: 0.81 - ETA: 19s - loss: 0.3958 - acc: 0.81 - ETA: 18s - loss: 0.3955 - acc: 0.81 - ETA: 17s - loss: 0.3952 - acc: 0.81 - ETA: 15s - loss: 0.3951 - acc: 0.81 - ETA: 14s - loss: 0.3946 - acc: 0.81 - ETA: 12s - loss: 0.3941 - acc: 0.81 - ETA: 11s - loss: 0.3938 - acc: 0.81 - ETA: 9s - loss: 0.3935 - acc: 0.8133 - ETA: 8s - loss: 0.3932 - acc: 0.813 - ETA: 7s - loss: 0.3932 - acc: 0.813 - ETA: 5s - loss: 0.3934 - acc: 0.813 - ETA: 4s - loss: 0.3931 - acc: 0.813 - ETA: 2s - loss: 0.3929 - acc: 0.813 - ETA: 1s - loss: 0.3926 - acc: 0.8139\n",
      "Epoch 00001: val_loss improved from inf to 0.31603, saving model to C:\\Users\\alizaidi\\tatk\\resources\\logs\\imdb_results\\2018_06_12\\kernel_3\\checkpoints\n",
      "450/450 [==============================] - 658s 1s/step - loss: 0.3924 - acc: 0.8139 - val_loss: 0.3160 - val_acc: 0.8564\n",
      "Epoch 2/15\n",
      " 92/450 [=====>........................] - ETA: 7:04 - loss: 0.3718 - acc: 0.860 - ETA: 8:28 - loss: 0.3064 - acc: 0.880 - ETA: 8:48 - loss: 0.2943 - acc: 0.866 - ETA: 7:43 - loss: 0.2737 - acc: 0.870 - ETA: 7:35 - loss: 0.2903 - acc: 0.864 - ETA: 7:44 - loss: 0.2801 - acc: 0.870 - ETA: 8:03 - loss: 0.2879 - acc: 0.862 - ETA: 7:55 - loss: 0.2815 - acc: 0.862 - ETA: 7:47 - loss: 0.2799 - acc: 0.864 - ETA: 7:52 - loss: 0.2721 - acc: 0.868 - ETA: 8:01 - loss: 0.2770 - acc: 0.869 - ETA: 7:58 - loss: 0.2795 - acc: 0.870 - ETA: 8:01 - loss: 0.2863 - acc: 0.867 - ETA: 7:55 - loss: 0.2878 - acc: 0.867 - ETA: 7:55 - loss: 0.2861 - acc: 0.866 - ETA: 7:50 - loss: 0.2852 - acc: 0.866 - ETA: 7:59 - loss: 0.2817 - acc: 0.871 - ETA: 8:01 - loss: 0.2775 - acc: 0.876 - ETA: 7:56 - loss: 0.2854 - acc: 0.874 - ETA: 8:04 - loss: 0.2817 - acc: 0.876 - ETA: 8:08 - loss: 0.2765 - acc: 0.880 - ETA: 8:20 - loss: 0.2723 - acc: 0.882 - ETA: 8:22 - loss: 0.2677 - acc: 0.886 - ETA: 8:38 - loss: 0.2664 - acc: 0.887 - ETA: 8:36 - loss: 0.2646 - acc: 0.887 - ETA: 8:33 - loss: 0.2589 - acc: 0.890 - ETA: 8:31 - loss: 0.2540 - acc: 0.893 - ETA: 8:36 - loss: 0.2525 - acc: 0.895 - ETA: 8:35 - loss: 0.2488 - acc: 0.897 - ETA: 8:29 - loss: 0.2517 - acc: 0.895 - ETA: 8:29 - loss: 0.2531 - acc: 0.894 - ETA: 8:25 - loss: 0.2561 - acc: 0.894 - ETA: 8:26 - loss: 0.2575 - acc: 0.892 - ETA: 8:24 - loss: 0.2592 - acc: 0.892 - ETA: 8:23 - loss: 0.2594 - acc: 0.893 - ETA: 8:26 - loss: 0.2600 - acc: 0.893 - ETA: 8:29 - loss: 0.2613 - acc: 0.894 - ETA: 8:32 - loss: 0.2603 - acc: 0.894 - ETA: 8:28 - loss: 0.2598 - acc: 0.894 - ETA: 8:22 - loss: 0.2629 - acc: 0.893 - ETA: 8:22 - loss: 0.2643 - acc: 0.891 - ETA: 8:22 - loss: 0.2633 - acc: 0.891 - ETA: 8:17 - loss: 0.2605 - acc: 0.892 - ETA: 8:19 - loss: 0.2606 - acc: 0.892 - ETA: 8:15 - loss: 0.2581 - acc: 0.893 - ETA: 8:14 - loss: 0.2593 - acc: 0.892 - ETA: 8:14 - loss: 0.2627 - acc: 0.890 - ETA: 8:11 - loss: 0.2627 - acc: 0.890 - ETA: 8:08 - loss: 0.2609 - acc: 0.891 - ETA: 8:08 - loss: 0.2622 - acc: 0.892 - ETA: 8:05 - loss: 0.2635 - acc: 0.891 - ETA: 8:00 - loss: 0.2631 - acc: 0.891 - ETA: 7:54 - loss: 0.2644 - acc: 0.890 - ETA: 7:51 - loss: 0.2651 - acc: 0.889 - ETA: 7:47 - loss: 0.2654 - acc: 0.888 - ETA: 7:46 - loss: 0.2672 - acc: 0.887 - ETA: 7:44 - loss: 0.2687 - acc: 0.885 - ETA: 7:44 - loss: 0.2678 - acc: 0.885 - ETA: 7:42 - loss: 0.2684 - acc: 0.884 - ETA: 7:40 - loss: 0.2701 - acc: 0.883 - ETA: 7:38 - loss: 0.2710 - acc: 0.883 - ETA: 7:38 - loss: 0.2717 - acc: 0.882 - ETA: 7:35 - loss: 0.2709 - acc: 0.882 - ETA: 7:35 - loss: 0.2730 - acc: 0.881 - ETA: 7:32 - loss: 0.2718 - acc: 0.882 - ETA: 7:30 - loss: 0.2718 - acc: 0.882 - ETA: 7:29 - loss: 0.2707 - acc: 0.883 - ETA: 7:29 - loss: 0.2693 - acc: 0.883 - ETA: 7:25 - loss: 0.2690 - acc: 0.883 - ETA: 7:24 - loss: 0.2680 - acc: 0.884 - ETA: 7:21 - loss: 0.2661 - acc: 0.884 - ETA: 7:19 - loss: 0.2668 - acc: 0.884 - ETA: 7:18 - loss: 0.2656 - acc: 0.885 - ETA: 7:17 - loss: 0.2661 - acc: 0.885 - ETA: 7:14 - loss: 0.2660 - acc: 0.884 - ETA: 7:12 - loss: 0.2642 - acc: 0.885 - ETA: 7:11 - loss: 0.2650 - acc: 0.885 - ETA: 7:11 - loss: 0.2654 - acc: 0.885 - ETA: 7:12 - loss: 0.2642 - acc: 0.886 - ETA: 7:10 - loss: 0.2650 - acc: 0.885 - ETA: 7:09 - loss: 0.2650 - acc: 0.885 - ETA: 7:07 - loss: 0.2659 - acc: 0.885 - ETA: 7:06 - loss: 0.2655 - acc: 0.885 - ETA: 7:05 - loss: 0.2648 - acc: 0.886 - ETA: 7:05 - loss: 0.2671 - acc: 0.885 - ETA: 7:05 - loss: 0.2656 - acc: 0.887 - ETA: 7:04 - loss: 0.2648 - acc: 0.887 - ETA: 7:02 - loss: 0.2665 - acc: 0.887 - ETA: 7:00 - loss: 0.2673 - acc: 0.886 - ETA: 7:00 - loss: 0.2677 - acc: 0.886 - ETA: 7:05 - loss: 0.2671 - acc: 0.887 - ETA: 7:04 - loss: 0.2661 - acc: 0.8876"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-0dd05ba5a6d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mkeras_text_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimdb_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\local\\AmlWorkbench\\Python\\lib\\site-packages\\tatk\\core\\base_text_classifier.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, **fit_params)\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_pipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_schema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\local\\AmlWorkbench\\Python\\lib\\site-packages\\tatk\\core\\tatk_pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, **fit_params)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtatk_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    500\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\local\\AmlWorkbench\\Python\\lib\\site-packages\\tatk\\core\\tatk_multi_col_estimator.py\u001b[0m in \u001b[0;36mtatk_fit\u001b[1;34m(self, dataset, **fit_params)\u001b[0m\n\u001b[0;32m    253\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_label_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 255\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\local\\AmlWorkbench\\Python\\lib\\site-packages\\tatk\\estimators\\keras_embedding_text_classifier_learner.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    199\u001b[0m             \u001b[0mval_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m         self.model.fit_generator(generator, steps_per_epoch=steps_per_epoch, epochs=self.n_epochs,\n\u001b[1;32m--> 201\u001b[1;33m                                  max_queue_size=10, callbacks=call_list, validation_data=val_generator, validation_steps=val_steps)\n\u001b[0m\u001b[0;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\local\\AmlWorkbench\\Python\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\local\\AmlWorkbench\\Python\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2175\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[0;32m   2176\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2177\u001b[1;33m                                                class_weight=class_weight)\n\u001b[0m\u001b[0;32m   2178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2179\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\local\\AmlWorkbench\\Python\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1847\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1848\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1849\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1850\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1851\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\local\\AmlWorkbench\\Python\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2475\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2476\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\local\\AmlWorkbench\\Python\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\local\\AmlWorkbench\\Python\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1135\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1137\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1138\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\local\\AmlWorkbench\\Python\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1353\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1355\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1356\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1357\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\local\\AmlWorkbench\\Python\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1359\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1360\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1361\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1362\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\local\\AmlWorkbench\\Python\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1338\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[1;32m-> 1340\u001b[1;33m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[0;32m   1341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "keras_text_classifier.fit(imdb_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Apply the text classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_test = pd.DataFrame({'review': X_test, 'sentiment': y_test})\n",
    "keras_text_classifier.predict(imdb_test)\n",
    "imdb_test[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = keras_text_classifier.evaluate(imdb_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.plot_confusion_matrix(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Save the pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a path to save your pipeline and model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_path = pathlib.Path(resources_dir) / \"models\" / datetime.datetime.now().strftime(\"%Y_%m_%d\") / \"keras_text_classifier_cnn\"\n",
    "if not pipeline_path.exists(): pipeline_path.mkdir(parents=True)\n",
    "print(pipeline_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_text_classifier.save(str(pipeline_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Load the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_keras_model = keras_text_classifier.load(str(pipeline_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_res = loaded_keras_model.evaluate(imdb_test)\n",
    "verify_res.plot_confusion_matrix(normalize=\"True\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify the same as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(res.cnf_matrix == verify_res.cnf_matrix).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can evaluate on new data. Let's try a hard example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict = pd.DataFrame({'review' : [\"This movie was so bad it was awesome\"]})\n",
    "df_res = loaded_keras_model.predict(df_predict)\n",
    "df_res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
