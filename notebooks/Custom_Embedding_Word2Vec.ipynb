{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AML Package for Text Analytics - Custom Word2Vec Embeddings\n",
    "### Training Word2Vec Embeddings Model on Domain-Specific Data\n",
    "\n",
    "This notebook shows how to use **Azure Machine Learning Package for Text Analytics** (AMLPTA) to train Word2Vec word embeddings on a large text corpus. \n",
    "\n",
    "The trained Word2Vec embeddings can be saved, and reloaded for later use to get embeddings of words or sentences in new datasets, where they will be used as features. \n",
    "\n",
    "A Word2Vec model, trained on a large corpus, learns representation of words that incorporates information about their context. The model is a simple neural network that predicts context words from a target word (skip-gram) or a target word from context words (Continuous Bag of Words CBOW). The first layer of the network acts like a lookup table, converting each input (ie. each word of the dictionary of the corpus) to a vector of fixed size, called embedding vector. This table of word vector representations is learned during training, which is done in an unsupervised fashion -- only text is supplied as input. The embedding table will be extracted after training and used for lookup and word similarity calculations. For example, it is used for lookup in pipelines in which word2vec features are used. \n",
    "\n",
    "In this notebook, a Word2Vec model training is implemented in the class `Word2VecModel`. It consists of the following steps:\n",
    "- `RegExTransformer`, which is used to clean the input text by removing certain patterns (regular expression).\n",
    "- `NltkPreprocessor`, which is used to tockenize and split the input in sentences.\n",
    "- `UngroupTransformer`, which ungroups the detected sentences by writing each one on a row of the dataset.\n",
    "- `Word2VecVectorizer`, which trains the FastText model.\n",
    "\n",
    "Note that `RegExTransformer` is only added to the pipeline if *regex* parameter is not *None* when creating a `Word2VecModel` instance, while `UngroupTransformer` is only added if *detect_sentences = True*.\n",
    "\n",
    "The model is trained on PubMed abstracts, grouped in 18 batches (tsv files of about 1 Gb each). Since the training on the whole data is time-consuming, in the context of this tutorial, training will be performed on a subset of the data.\n",
    "\n",
    "Following are the steps for creating a custom word2vec model using the package:\n",
    "<br> Step 1: Configure and import modules\n",
    "<br> Step 2: Prepare data for modeling and evaluation\n",
    "<br> Step 3: Train the Word2Vec model \n",
    "<br> Step 4: Save and load pipeline for additional training\n",
    "<br> Step 5: Save and load embeddings for lookup \n",
    "\n",
    "Consult the [package reference documentation](https://aka.ms/aml-packages/text) for the detailed reference for each module and class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites \n",
    "\n",
    "1. If you don't have an Azure subscription, create a [free account](https://azure.microsoft.com/free/?WT.mc_id=A261C142F) before you begin.\n",
    "\n",
    "1. The following accounts and application must be set up and installed:\n",
    "   - Azure Machine Learning Experimentation account \n",
    "   - An Azure Machine Learning Model Management account\n",
    "   - Azure Machine Learning Workbench installed\n",
    "\n",
    "   If these three are not yet created or installed, follow the [Azure Machine Learning Quickstart and Workbench installation](../service/quickstart-installation.md) article. \n",
    "\n",
    "1. The Azure Machine Learning Package for Text Analytics must be installed. Learn how to [install this package here](https://aka.ms/aml-packages/text)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Configure and import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Metadata-Version: 2.0\n",
      "Name: azureml-tatk\n",
      "Version: 0.1.18129.4a1\n",
      "Summary: Microsoft Azure Machine Learning Package for Text Analytics\n",
      "Home-page: https://microsoft.sharepoint.com/teams/TextAnalyticsPackagePreview\n",
      "Author: Microsoft Corporation\n",
      "Author-email: amltap@microsoft.com\n",
      "Installer: pip\n",
      "License: UNKNOWN\n",
      "Location: c:\\users\\tatk\\appdata\\local\\amlworkbench\\python\\lib\\site-packages\n",
      "Requires: unidecode, ipython, jsonpickle, matplotlib, keras, dill, pandas, ruamel.yaml, ipywidgets, nltk, pdfminer.six, pyspark, azure-ml-api-sdk, requests, qgrid, scipy, azure-storage, nose, h5py, bqplot, pytest, gensim, lxml, sklearn-crfsuite, docker, numpy, validators, scikit-learn\n",
      "Classifiers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 8.1.2, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import pip\n",
    "print(pip.main([\"show\",\"azureml-tatk\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.1) Configure AzureML logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<azureml.logging.script_run_request.ScriptRunRequest at 0x2518b309e80>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Azure Machine Learning logger and magics for logging and run-history tracking\n",
    "# Use the Azure Machine Learning data collector to log various metrics\n",
    "from azureml.logging import get_azureml_logger\n",
    "logger = get_azureml_logger()\n",
    "\n",
    "# Log cell runs into run history\n",
    "logger.log('Cell','Set up run')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.2) Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\tatk\\\\tatk\\\\resources\\\\models'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tatk.utils import models_dir\n",
    "from tatk.pipelines.feature_extraction.word2vec_model import Word2VecModel\n",
    "from tatk.feature_extraction.word2vec_vectorizer import Word2VecVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Prepare data for modeling and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.1) Download Pubmed data. \n",
    "Please download the raw MEDLINE abstract data from [MEDLINE](https://www.nlm.nih.gov/pubs/factsheets/medline.html). The data is publicly available in the form of XML files on their [FTP server](https://ftp.ncbi.nlm.nih.gov/pubmed/baseline). There are 892 XML files available on the server and each of the XML files has the information of 30,000 articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set `xml_local_dir` to point to the folder containing downloaded xml files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_local_dir = r'C:\\Users\\tatk\\Downloads\\pubmed\\raw'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.2) Parse the downloaded xml data.\n",
    "\n",
    "You can use the Azure ML Text Analytics Package to parse the downloaded xml Pubmed files. It will read the xml files per batches (see batch_size argument), preprocess them (including removing articles with short abstracts; see min_abstract_len argument), keep specific columns (see cols_to_keep argument), and save them to disk in tsv files in tsv_local_dir.\n",
    "\n",
    "Reading and saving xml files in batches will let you train the embeddings model incrementally, instead of reading in-memory a big amount of data.\n",
    "\n",
    "You can change `tsv_local_dir` to specify another path to the output preprocessed data.\n",
    "\n",
    "Change `num_xml_files` based on how many xml files you downloaded and/or want to process.\n",
    "\n",
    "Change `batch_size` based on how many xml files you want to concatenate into 1 tsv file for the model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process downloaded MEDLINE XML files - start\n",
      "TSV directory C:\\Users\\tatk\\Downloads\\pubmed\\processed\n",
      "Processing 10 files ....\n",
      "\tprocessing pubmed18n0001.xml.gz .....\n",
      "\tnumber of records before missing data removal = 30000\n",
      "RegExTransformer::tatk_transform ==> start\n",
      "RegExTransformer::tatk_transform ==> end \t Time taken: 0.02 mins\n",
      "\tnumber of records after missing data removal = 15377\n",
      "\tprocessing pubmed18n0002.xml.gz .....\n",
      "\tnumber of records before missing data removal = 30000\n",
      "RegExTransformer::tatk_transform ==> start\n",
      "RegExTransformer::tatk_transform ==> end \t Time taken: 0.02 mins\n",
      "\tnumber of records after missing data removal = 13414\n",
      "\tprocessing pubmed18n0003.xml.gz .....\n",
      "\tnumber of records before missing data removal = 30000\n",
      "RegExTransformer::tatk_transform ==> start\n",
      "RegExTransformer::tatk_transform ==> end \t Time taken: 0.02 mins\n",
      "\tnumber of records after missing data removal = 12615\n",
      "\tprocessing pubmed18n0004.xml.gz .....\n",
      "\tnumber of records before missing data removal = 30000\n",
      "RegExTransformer::tatk_transform ==> start\n",
      "RegExTransformer::tatk_transform ==> end \t Time taken: 0.02 mins\n",
      "\tnumber of records after missing data removal = 14941\n",
      "\tprocessing pubmed18n0005.xml.gz .....\n",
      "\tnumber of records before missing data removal = 30000\n",
      "RegExTransformer::tatk_transform ==> start\n",
      "RegExTransformer::tatk_transform ==> end \t Time taken: 0.02 mins\n",
      "\tnumber of records after missing data removal = 14078\n",
      "\tprocessing pubmed18n0006.xml.gz .....\n",
      "\tnumber of records before missing data removal = 30000\n",
      "RegExTransformer::tatk_transform ==> start\n",
      "RegExTransformer::tatk_transform ==> end \t Time taken: 0.03 mins\n",
      "\tnumber of records after missing data removal = 16336\n",
      "\tprocessing pubmed18n0007.xml.gz .....\n",
      "\tnumber of records before missing data removal = 30000\n",
      "RegExTransformer::tatk_transform ==> start\n",
      "RegExTransformer::tatk_transform ==> end \t Time taken: 0.03 mins\n",
      "\tnumber of records after missing data removal = 16386\n",
      "\tprocessing pubmed18n0008.xml.gz .....\n",
      "\tnumber of records before missing data removal = 30000\n",
      "RegExTransformer::tatk_transform ==> start\n",
      "RegExTransformer::tatk_transform ==> end \t Time taken: 0.02 mins\n",
      "\tnumber of records after missing data removal = 15373\n",
      "\tprocessing pubmed18n0009.xml.gz .....\n",
      "\tnumber of records before missing data removal = 30000\n",
      "RegExTransformer::tatk_transform ==> start\n",
      "RegExTransformer::tatk_transform ==> end \t Time taken: 0.01 mins\n",
      "\tnumber of records after missing data removal = 5500\n",
      "\tprocessing pubmed18n0010.xml.gz .....\n",
      "\tnumber of records before missing data removal = 30000\n",
      "RegExTransformer::tatk_transform ==> start\n",
      "RegExTransformer::tatk_transform ==> end \t Time taken: 0.01 mins\n",
      "\tnumber of records after missing data removal = 9356\n",
      "Writing 29923 records to file C:\\Users\\tatk\\Downloads\\pubmed\\processed\\batch#1.tsv .....\n",
      "Processing 10 files ....\n",
      "\tprocessing pubmed18n0011.xml.gz .....\n",
      "\tnumber of records before missing data removal = 30000\n",
      "RegExTransformer::tatk_transform ==> start\n",
      "RegExTransformer::tatk_transform ==> end \t Time taken: 0.02 mins\n",
      "\tnumber of records after missing data removal = 13476\n",
      "\tprocessing pubmed18n0012.xml.gz .....\n",
      "\tnumber of records before missing data removal = 30000\n",
      "RegExTransformer::tatk_transform ==> start\n",
      "RegExTransformer::tatk_transform ==> end \t Time taken: 0.02 mins\n",
      "\tnumber of records after missing data removal = 11831\n",
      "\tprocessing pubmed18n0013.xml.gz .....\n",
      "\tnumber of records before missing data removal = 30000\n",
      "RegExTransformer::tatk_transform ==> start\n",
      "RegExTransformer::tatk_transform ==> end \t Time taken: 0.02 mins\n",
      "\tnumber of records after missing data removal = 11759\n",
      "\tprocessing pubmed18n0014.xml.gz .....\n",
      "\tnumber of records before missing data removal = 30000\n",
      "RegExTransformer::tatk_transform ==> start\n",
      "RegExTransformer::tatk_transform ==> end \t Time taken: 0.02 mins\n",
      "\tnumber of records after missing data removal = 14832\n",
      "\tprocessing pubmed18n0015.xml.gz .....\n",
      "\tnumber of records before missing data removal = 30000\n",
      "RegExTransformer::tatk_transform ==> start\n",
      "RegExTransformer::tatk_transform ==> end \t Time taken: 0.02 mins\n",
      "\tnumber of records after missing data removal = 13818\n",
      "\tprocessing pubmed18n0016.xml.gz .....\n",
      "\tnumber of records before missing data removal = 30000\n",
      "RegExTransformer::tatk_transform ==> start\n",
      "RegExTransformer::tatk_transform ==> end \t Time taken: 0.02 mins\n",
      "\tnumber of records after missing data removal = 14036\n",
      "\tprocessing pubmed18n0017.xml.gz .....\n",
      "\tnumber of records before missing data removal = 30000\n",
      "RegExTransformer::tatk_transform ==> start\n",
      "RegExTransformer::tatk_transform ==> end \t Time taken: 0.02 mins\n",
      "\tnumber of records after missing data removal = 12634\n",
      "\tprocessing pubmed18n0018.xml.gz .....\n",
      "\tnumber of records before missing data removal = 30000\n",
      "RegExTransformer::tatk_transform ==> start\n",
      "RegExTransformer::tatk_transform ==> end \t Time taken: 0.02 mins\n",
      "\tnumber of records after missing data removal = 11461\n",
      "\tprocessing pubmed18n0019.xml.gz .....\n",
      "\tnumber of records before missing data removal = 30000\n",
      "RegExTransformer::tatk_transform ==> start\n",
      "RegExTransformer::tatk_transform ==> end \t Time taken: 0.02 mins\n",
      "\tnumber of records after missing data removal = 13656\n",
      "\tprocessing pubmed18n0020.xml.gz .....\n",
      "\tnumber of records before missing data removal = 30000\n",
      "RegExTransformer::tatk_transform ==> start\n",
      "RegExTransformer::tatk_transform ==> end \t Time taken: 0.01 mins\n",
      "\tnumber of records after missing data removal = 10242\n",
      "Writing 29894 records to file C:\\Users\\tatk\\Downloads\\pubmed\\processed\\batch#2.tsv .....\n",
      "Process downloaded MEDLINE XML files - end\n"
     ]
    }
   ],
   "source": [
    "from tatk.utils.parse_pubmed_data import process_files\n",
    "\n",
    "tsv_local_dir = r'C:\\Users\\tatk\\Downloads\\pubmed\\processed'\n",
    "num_xml_files = 20\n",
    "batch_size = 10 \n",
    "\n",
    "process_files(xml_local_dir = xml_local_dir,\n",
    "              tsv_local_dir =tsv_local_dir,\n",
    "              batch_size=batch_size,\n",
    "              num_xml_files=num_xml_files,\n",
    "              min_abstract_len=10,\n",
    "              cols_to_keep=['pmid', 'abstract'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.3) Read the first batch in-memory. \n",
    "Each paper has an ID (pmid) and a text abstract. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(tsv_local_dir, 'batch#1.tsv')\n",
    "\n",
    "data = pd.read_csv(file_path, sep = \"\\t\", usecols = ['pmid', 'abstract'], encoding = \"ISO-8859-1\").dropna()#read in-memory\n",
    "df = data[:5000] # take a subset for faster training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pmid</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21.0</td>\n",
       "      <td>(--)-alpha-Bisabolol has a primary antipeptic ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22.0</td>\n",
       "      <td>A report is given on the recent discovery of o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23.0</td>\n",
       "      <td>The distribution of blood flow to the subendoc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24.0</td>\n",
       "      <td>The virostatic compound N,N-diethyl-4-[2-(2-ox...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25.0</td>\n",
       "      <td>RMI 61 140, RMI 61 144 and RMI 61 280 are newl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    pmid                                           abstract\n",
       "20  21.0  (--)-alpha-Bisabolol has a primary antipeptic ...\n",
       "21  22.0  A report is given on the recent discovery of o...\n",
       "22  23.0  The distribution of blood flow to the subendoc...\n",
       "23  24.0  The virostatic compound N,N-diethyl-4-[2-(2-ox...\n",
       "24  25.0  RMI 61 140, RMI 61 144 and RMI 61 280 are newl..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train the Word2Vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3.1) Create the Word2Vec model pipeline\n",
    "Initialize the pipeline with default parameters. No regular expression cleaning is performed, and sentences are detected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2VecModel::create_pipeline ==> start\n",
      "input_col=abstract\n",
      "input_col=NltkPreprocessor6a6fd0bac3c54285b1fa0f4fb761c8ec\n",
      "input_col=UngroupTransformerf7c4646520a84945ae49b853c24e4b26\n",
      ":: number of jobs for the pipeline : 6\n",
      "0\tnltk_preprocessor\n",
      "1\tungroup_transformer\n",
      "2\tvectorizer\n",
      "Word2VecModel::create_pipeline ==> end\n"
     ]
    }
   ],
   "source": [
    "word2vec_model = Word2VecModel(input_col = 'abstract', regex = None, detect_sentences = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2VecModel TATK Pipeline:\n",
      "0 - nltk_preprocessor(abstract,NltkPreprocessor6a6fd0bac3c54285b1fa0f4fb761c8ec)\n",
      "1 - ungroup_transformer(NltkPreprocessor6a6fd0bac3c54285b1fa0f4fb761c8ec,UngroupTransformerf7c4646520a84945ae49b853c24e4b26)\n",
      "2 - vectorizer(UngroupTransformerf7c4646520a84945ae49b853c24e4b26,Word2VecVectorizer6b41f68986cd47e39a887e1df7ad3c1e)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(word2vec_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3.2) Display and Change default pipeline parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aggregation_func': <function tatk.feature_extraction.word2vec_vectorizer.Word2VecVectorizer.aggregate_mean(sentence_matrix)>,\n",
       " 'case_sensitive': False,\n",
       " 'context_window_size': 5,\n",
       " 'copy_from_path': True,\n",
       " 'embedding_size': 100,\n",
       " 'embedding_table': None,\n",
       " 'get_from_path': True,\n",
       " 'input_col': 'UngroupTransformerf7c4646520a84945ae49b853c24e4b26',\n",
       " 'lr_end': 0.005,\n",
       " 'lr_start': 0.05,\n",
       " 'min_df': 5,\n",
       " 'negative_sample_size': 5,\n",
       " 'num_epochs': 5,\n",
       " 'num_workers': 4,\n",
       " 'output_col': 'Word2VecVectorizer6b41f68986cd47e39a887e1df7ad3c1e',\n",
       " 'return_type': 'word_vector',\n",
       " 'save_overwrite': True,\n",
       " 'skip_OOV': False,\n",
       " 'trainable': True,\n",
       " 'trained_model': None,\n",
       " 'use_hierarchical_softmax': 0,\n",
       " 'use_skipgram': 0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.get_step_params_by_name('vectorizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change model parameters\n",
    "word2vec_model.set_step_params_by_name('vectorizer', use_skipgram = 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3.3) Fit the model on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2VecModel::fit ==> start\n",
      "schema: col=pmid:R4:0 col=abstract:TX:1 header+\n",
      "NltkPreprocessor::tatk_fit_transform ==> start\n",
      "NltkPreprocessor::tatk_fit_transform ==> end \t Time taken: 0.05 mins\n",
      "UngroupTransformer::tatk_fit_transform ==> start\n",
      "UngroupTransformer::tatk_fit_transform ==> end \t Time taken: 0.0 mins\n",
      "Word2VecVectorizer::tatk_fit ==> start\n",
      "collecting all words and their counts\n",
      "PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "PROGRESS: at sentence #10000, processed 265322 words, keeping 15654 word types\n",
      "PROGRESS: at sentence #20000, processed 527867 words, keeping 22727 word types\n",
      "PROGRESS: at sentence #30000, processed 792813 words, keeping 28054 word types\n",
      "collected 30449 word types from a corpus of 917752 raw words and 34803 sentences\n",
      "Loading a fresh vocabulary\n",
      "min_count=5 retains 9697 unique words (31% of original 30449, drops 20752)\n",
      "min_count=5 leaves 883549 word corpus (96% of original 917752, drops 34203)\n",
      "deleting the raw counts dictionary of 30449 items\n",
      "sample=0.0001 downsamples 462 most-common words\n",
      "downsampling leaves estimated 422083 word corpus (47.8% of prior 883549)\n",
      "estimated required memory for 9697 words and 100 dimensions: 12606100 bytes\n",
      "resetting layer weights\n",
      "training model with 4 workers on 9697 vocabulary and 100 features, using sg=1 hs=0 sample=0.0001 negative=5 window=5\n",
      "PROGRESS: at 26.33% examples, 553794 words/s, in_qsize 7, out_qsize 0\n",
      "PROGRESS: at 53.28% examples, 560509 words/s, in_qsize 7, out_qsize 0\n",
      "PROGRESS: at 80.70% examples, 563002 words/s, in_qsize 8, out_qsize 0\n",
      "worker thread finished; awaiting finish of 3 more threads\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "training on 4588760 raw words (2111005 effective words) took 3.7s, 564975 effective words/s\n",
      "vocabulary size =9697\n",
      "Word2VecVectorizer::tatk_fit ==> end \t Time taken: 0.08 mins\n",
      "Time taken: 0.13 mins\n",
      "Word2VecModel::fit ==> end\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Word2VecModel(detect_sentences=True, input_col='abstract', regex=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3.4) Script to train the embeddings on multiple batches (note: very long on full 21 Gb of data).\n",
    "We loop over the different batches in tsv_local_dir, read every file in-memory, and feed it to the model for incremental learning.\n",
    "\n",
    "Replace *False* by *True* if you would like to perform this step. Otherwise, proceed with the model trained above on a subset of batch #1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2VecModel::create_pipeline ==> start\n",
      "input_col=abstract\n",
      "input_col=NltkPreprocessor527ddb5170cd41728a90cd72a46d4d41\n",
      "input_col=UngroupTransformeree8af7b41ea344379dbf9803e6ee5fbb\n",
      ":: number of jobs for the pipeline : 6\n",
      "0\tnltk_preprocessor\n",
      "1\tungroup_transformer\n",
      "2\tvectorizer\n",
      "Word2VecModel::create_pipeline ==> end\n",
      "1\n",
      "(15377, 1)\n",
      "Word2VecModel::fit ==> start\n",
      "schema: col=abstract:TX:0 header+\n",
      "NltkPreprocessor::tatk_fit_transform ==> start\n",
      "NltkPreprocessor::tatk_fit_transform ==> end \t Time taken: 0.15 mins\n",
      "UngroupTransformer::tatk_fit_transform ==> start\n",
      "UngroupTransformer::tatk_fit_transform ==> end \t Time taken: 0.0 mins\n",
      "Word2VecVectorizer::tatk_fit ==> start\n",
      "collecting all words and their counts\n",
      "PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "PROGRESS: at sentence #10000, processed 265322 words, keeping 15654 word types\n",
      "PROGRESS: at sentence #20000, processed 527867 words, keeping 22727 word types\n",
      "PROGRESS: at sentence #30000, processed 792813 words, keeping 28054 word types\n",
      "PROGRESS: at sentence #40000, processed 1053779 words, keeping 32657 word types\n",
      "PROGRESS: at sentence #50000, processed 1315882 words, keeping 36799 word types\n",
      "PROGRESS: at sentence #60000, processed 1578471 words, keeping 40372 word types\n",
      "PROGRESS: at sentence #70000, processed 1835736 words, keeping 44090 word types\n",
      "PROGRESS: at sentence #80000, processed 2099625 words, keeping 47463 word types\n",
      "PROGRESS: at sentence #90000, processed 2360633 words, keeping 50436 word types\n",
      "PROGRESS: at sentence #100000, processed 2624493 words, keeping 53257 word types\n",
      "collected 54151 word types from a corpus of 2714598 raw words and 103388 sentences\n",
      "Loading a fresh vocabulary\n",
      "min_count=5 retains 17376 unique words (32% of original 54151, drops 36775)\n",
      "min_count=5 leaves 2655037 word corpus (97% of original 2714598, drops 59561)\n",
      "deleting the raw counts dictionary of 54151 items\n",
      "sample=0.0001 downsamples 448 most-common words\n",
      "downsampling leaves estimated 1303703 word corpus (49.1% of prior 2655037)\n",
      "estimated required memory for 17376 words and 100 dimensions: 22588800 bytes\n",
      "resetting layer weights\n",
      "training model with 4 workers on 17376 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=5\n",
      "PROGRESS: at 19.43% examples, 1264052 words/s, in_qsize 7, out_qsize 0\n",
      "PROGRESS: at 39.44% examples, 1281683 words/s, in_qsize 7, out_qsize 0\n",
      "PROGRESS: at 58.71% examples, 1272647 words/s, in_qsize 7, out_qsize 0\n",
      "PROGRESS: at 77.63% examples, 1258060 words/s, in_qsize 7, out_qsize 0\n",
      "PROGRESS: at 96.66% examples, 1254379 words/s, in_qsize 7, out_qsize 0\n",
      "worker thread finished; awaiting finish of 3 more threads\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "training on 13572990 raw words (6519396 effective words) took 5.2s, 1254096 effective words/s\n",
      "vocabulary size =17376\n",
      "Word2VecVectorizer::tatk_fit ==> end \t Time taken: 0.12 mins\n",
      "Time taken: 0.27 mins\n",
      "Word2VecModel::fit ==> end\n",
      "2\n",
      "(13476, 1)\n",
      "Word2VecModel::fit ==> start\n",
      "schema: col=abstract:TX:0 header+\n",
      "NltkPreprocessor::tatk_fit_transform ==> start\n",
      "NltkPreprocessor::tatk_fit_transform ==> end \t Time taken: 0.12 mins\n",
      "UngroupTransformer::tatk_fit_transform ==> start\n",
      "UngroupTransformer::tatk_fit_transform ==> end \t Time taken: 0.0 mins\n",
      "Word2VecVectorizer::tatk_fit ==> start\n",
      "collecting all words and their counts\n",
      "PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "PROGRESS: at sentence #10000, processed 255350 words, keeping 15266 word types\n",
      "PROGRESS: at sentence #20000, processed 513032 words, keeping 21222 word types\n",
      "PROGRESS: at sentence #30000, processed 776612 words, keeping 25715 word types\n",
      "PROGRESS: at sentence #40000, processed 1013554 words, keeping 29958 word types\n",
      "PROGRESS: at sentence #50000, processed 1271864 words, keeping 35191 word types\n",
      "PROGRESS: at sentence #60000, processed 1532207 words, keeping 39573 word types\n",
      "PROGRESS: at sentence #70000, processed 1790798 words, keeping 43411 word types\n",
      "PROGRESS: at sentence #80000, processed 2049144 words, keeping 46985 word types\n",
      "collected 47428 word types from a corpus of 2076879 raw words and 81076 sentences\n",
      "Updating model with new vocabulary\n",
      "New added 15985 unique words (25% of original 63413) and increased the count of 15985 pre-existing words (25% of original 63413)\n",
      "deleting the raw counts dictionary of 47428 items\n",
      "sample=0.0001 downsamples 898 most-common words\n",
      "downsampling leaves estimated 2051848 word corpus (101.3% of prior 2024777)\n",
      "estimated required memory for 31970 words and 100 dimensions: 41561000 bytes\n",
      "updating layer weights\n",
      "training model with 4 workers on 21903 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=5\n",
      "PROGRESS: at 23.75% examples, 1224632 words/s, in_qsize 7, out_qsize 0\n",
      "PROGRESS: at 47.24% examples, 1219985 words/s, in_qsize 7, out_qsize 0\n",
      "PROGRESS: at 71.16% examples, 1222011 words/s, in_qsize 7, out_qsize 0\n",
      "PROGRESS: at 94.01% examples, 1208953 words/s, in_qsize 7, out_qsize 0\n",
      "worker thread finished; awaiting finish of 3 more threads\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "training on 10384395 raw words (5167330 effective words) took 4.3s, 1202249 effective words/s\n",
      "vocabulary size =21903\n",
      "Word2VecVectorizer::tatk_fit ==> end \t Time taken: 0.1 mins\n",
      "Time taken: 0.22 mins\n",
      "Word2VecModel::fit ==> end\n"
     ]
    }
   ],
   "source": [
    "num_batches = 2\n",
    "if False:\n",
    "    word2vec_model = Word2VecModel(input_col = 'abstract', regex = None, detect_sentences = True)#Initialize the model.\n",
    "    for b in range(1, num_batches + 1):\n",
    "        print(b)\n",
    "        file_path = os.path.join(tsv_local_dir, 'batch#{}.tsv'.format(b))\n",
    "        df = pd.read_csv(file_path, sep = \"\\t\", usecols = ['abstract'], encoding = \"ISO-8859-1\").dropna()\n",
    "        print(df.shape)\n",
    "        word2vec_model.fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Save and load pipeline for additional training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4.1) Save and Load the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseTextModel::save ==> start\n",
      "TatkPipeline::save ==> start\n",
      "saving Word2Vec object under C:\\Users\\tatk\\tatk\\resources\\models\\word2vec_model\\pipeline\\vectorizer\\embedding_model.gen, separately None\n",
      "not storing attribute syn0norm\n",
      "not storing attribute cum_table\n",
      "saved C:\\Users\\tatk\\tatk\\resources\\models\\word2vec_model\\pipeline\\vectorizer\\embedding_model.gen\n",
      "Time taken: 0.01 mins\n",
      "TatkPipeline::save ==> end\n",
      "Time taken: 0.01 mins\n",
      "BaseTextModel::save ==> end\n",
      "BaseTextModel::load ==> start\n",
      "TatkPipeline::load ==> start\n",
      "loading Word2Vec object from C:\\Users\\tatk\\tatk\\resources\\models\\word2vec_model\\pipeline\\vectorizer\\embedding_model.gen\n",
      "loading wv recursively from C:\\Users\\tatk\\tatk\\resources\\models\\word2vec_model\\pipeline\\vectorizer\\embedding_model.gen.wv.* with mmap=None\n",
      "setting ignored attribute syn0norm to None\n",
      "setting ignored attribute cum_table to None\n",
      "loaded C:\\Users\\tatk\\tatk\\resources\\models\\word2vec_model\\pipeline\\vectorizer\\embedding_model.gen\n",
      "Word2VecVectorizer: Word2Vec model loaded from C:\\Users\\tatk\\tatk\\resources\\models\\word2vec_model\\pipeline\\vectorizer\\embedding_model.gen\n",
      "Time taken: 0.0 mins\n",
      "TatkPipeline::load ==> end\n",
      "Time taken: 0.0 mins\n",
      "BaseTextModel::load ==> end\n"
     ]
    }
   ],
   "source": [
    "pipeline_path = os.path.join(models_dir, 'word2vec_model')\n",
    "word2vec_model.save(pipeline_path, create_folders_on_path=True)\n",
    "word2vec_model2 = Word2VecModel.load(pipeline_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4.2) Perform additional training on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2VecModel::fit ==> start\n",
      "schema: col=pmid:R4:0 col=abstract:TX:1 header+\n",
      "NltkPreprocessor::tatk_fit_transform ==> start\n",
      "NltkPreprocessor::tatk_fit_transform ==> end \t Time taken: 0.01 mins\n",
      "UngroupTransformer::tatk_fit_transform ==> start\n",
      "UngroupTransformer::tatk_fit_transform ==> end \t Time taken: 0.0 mins\n",
      "Word2VecVectorizer::tatk_fit ==> start\n",
      "collecting all words and their counts\n",
      "PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "collected 13179 word types from a corpus of 184248 raw words and 7106 sentences\n",
      "Updating model with new vocabulary\n",
      "New added 3795 unique words (22% of original 16974) and increased the count of 3795 pre-existing words (22% of original 16974)\n",
      "deleting the raw counts dictionary of 13179 items\n",
      "sample=0.0001 downsamples 976 most-common words\n",
      "downsampling leaves estimated 150507 word corpus (89.3% of prior 168598)\n",
      "estimated required memory for 7590 words and 100 dimensions: 9867000 bytes\n",
      "updating layer weights\n",
      "training model with 4 workers on 21903 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=5\n",
      "worker thread finished; awaiting finish of 3 more threads\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "training on 921240 raw words (436940 effective words) took 0.4s, 1137526 effective words/s\n",
      "vocabulary size =21903\n",
      "Word2VecVectorizer::tatk_fit ==> end \t Time taken: 0.01 mins\n",
      "Time taken: 0.02 mins\n",
      "Word2VecModel::fit ==> end\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Word2VecModel(detect_sentences=True, input_col='abstract', regex=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.DataFrame(data[5000:6000])\n",
    "word2vec_model2.fit(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Save and load embeddings for lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5.1) Save the embeddings from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2VecVectorizer::save_embeddings ==> start\n",
      "storing 21903x100 projection weights into C:\\Users\\tatk\\tatk\\resources\\models\\word2vec_embeddings.txt\n",
      "Time taken: 0.02 mins\n",
      "Word2VecVectorizer::save_embeddings ==> end\n"
     ]
    }
   ],
   "source": [
    "# Saved embeddings file is in textual format and is readable if opened with a text editor\n",
    "embeddings_file_path = os.path.join(models_dir, 'word2vec_embeddings.txt')\n",
    "word2vec_model2.save_embeddings(embeddings_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5.2) Load the embeddings to memory with include_unk set to True to add OOV treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2VecVectorizer::load_embeddings ==> start\n",
      "loading projection weights from C:\\Users\\tatk\\tatk\\resources\\models\\word2vec_embeddings.txt\n",
      "loaded (21903, 100) matrix from C:\\Users\\tatk\\tatk\\resources\\models\\word2vec_embeddings.txt\n",
      "Time taken: 0.04 mins\n",
      "Word2VecVectorizer::load_embeddings ==> end\n"
     ]
    }
   ],
   "source": [
    "vectorizer = Word2VecVectorizer.load_embeddings(embeddings_file_path, include_unk = True,\n",
    "                                                unk_method = 'rnd', unk_vector = None, unk_word = '<UNK>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5.3) Embedding Lookup: Get word and subword indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2VecVectorizer::tatk_transform ==> start\n",
      "Word2VecVectorizer::tatk_transform ==> end \t Time taken: 0.0 mins\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>indices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have fever</td>\n",
       "      <td>[125, 53, 1891]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My doctor prescribed me ibuprofen.</td>\n",
       "      <td>[10249, 7005, 4920, 3623, 21903]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 text                           indices\n",
       "0                        I have fever                   [125, 53, 1891]\n",
       "1  My doctor prescribed me ibuprofen.  [10249, 7005, 4920, 3623, 21903]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_predict = pd.DataFrame({'text' : [\"I have fever\", \"My doctor prescribed me ibuprofen.\"]})\n",
    "vectorizer.input_col = 'text'\n",
    "vectorizer.output_col = 'indices'\n",
    "vectorizer.return_type = 'word_index'\n",
    "result = vectorizer.tatk_transform(df_predict)\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5.4) Embedding Lookup: Get word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2VecVectorizer::tatk_transform ==> start\n",
      "Word2VecVectorizer::tatk_transform ==> end \t Time taken: 0.0 mins\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>indices</th>\n",
       "      <th>word_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have fever</td>\n",
       "      <td>[125, 53, 1891]</td>\n",
       "      <td>[[0.0705619975924, -0.0768989995122, 1.8238869...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My doctor prescribed me ibuprofen.</td>\n",
       "      <td>[10249, 7005, 4920, 3623, 21903]</td>\n",
       "      <td>[[0.075989998877, 0.00504799978808, -0.1458210...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 text                           indices  \\\n",
       "0                        I have fever                   [125, 53, 1891]   \n",
       "1  My doctor prescribed me ibuprofen.  [10249, 7005, 4920, 3623, 21903]   \n",
       "\n",
       "                                         word_vector  \n",
       "0  [[0.0705619975924, -0.0768989995122, 1.8238869...  \n",
       "1  [[0.075989998877, 0.00504799978808, -0.1458210...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vectorizer.output_col = 'word_vector'\n",
    "vectorizer.return_type = 'word_vector'\n",
    "result = vectorizer.tatk_transform(df_predict)\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5.5) Embedding Lookup: Get sentence embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2VecVectorizer::tatk_transform ==> start\n",
      "Word2VecVectorizer::tatk_transform ==> end \t Time taken: 0.0 mins\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>indices</th>\n",
       "      <th>word_vector</th>\n",
       "      <th>sentence_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have fever</td>\n",
       "      <td>[125, 53, 1891]</td>\n",
       "      <td>[[0.0705619975924, -0.0768989995122, 1.8238869...</td>\n",
       "      <td>[0.0703796669841, 0.10862232993, 0.62739165624...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My doctor prescribed me ibuprofen.</td>\n",
       "      <td>[10249, 7005, 4920, 3623, 21903]</td>\n",
       "      <td>[[0.075989998877, 0.00504799978808, -0.1458210...</td>\n",
       "      <td>[0.49121833265, 0.210629751274, -0.23683439670...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 text                           indices  \\\n",
       "0                        I have fever                   [125, 53, 1891]   \n",
       "1  My doctor prescribed me ibuprofen.  [10249, 7005, 4920, 3623, 21903]   \n",
       "\n",
       "                                         word_vector  \\\n",
       "0  [[0.0705619975924, -0.0768989995122, 1.8238869...   \n",
       "1  [[0.075989998877, 0.00504799978808, -0.1458210...   \n",
       "\n",
       "                                     sentence_vector  \n",
       "0  [0.0703796669841, 0.10862232993, 0.62739165624...  \n",
       "1  [0.49121833265, 0.210629751274, -0.23683439670...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vectorizer.output_col = 'sentence_vector'\n",
    "vectorizer.return_type = 'sentence_vector'\n",
    "result = vectorizer.tatk_transform(df_predict)\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5.6) Embedding Lookup: Get most similar word to a given word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('diarrhea', 0.7121096849441528),\n",
       " ('febrile', 0.6585642099380493),\n",
       " ('meningitis', 0.658104658126831),\n",
       " ('endophthalmitis', 0.6556650400161743),\n",
       " ('dysentery', 0.646338939666748),\n",
       " ('fatal', 0.6434411406517029),\n",
       " ('necrotizing', 0.6364932060241699),\n",
       " ('eosinophilia', 0.6359970569610596),\n",
       " ('diarrhoea', 0.6343546509742737),\n",
       " ('rash', 0.6313232183456421)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.embedding_table.most_similar('fever')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('physician', 0.8832104206085205),\n",
       " ('education', 0.846272349357605),\n",
       " ('physicians', 0.8457393050193787),\n",
       " ('assistants', 0.8402310609817505),\n",
       " ('staff', 0.8373662233352661),\n",
       " ('insurance', 0.8310399055480957),\n",
       " ('attitudes', 0.8197391033172607),\n",
       " ('nurse', 0.8195598125457764),\n",
       " ('assistant', 0.8187754154205322),\n",
       " ('practitioner', 0.8185635805130005)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.embedding_table.most_similar('doctor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('diphenhydramine', 0.8111883401870728),\n",
       " ('phenylbutazone', 0.7920838594436646),\n",
       " ('asa', 0.7769595384597778),\n",
       " ('flurbiprofen', 0.7701882719993591),\n",
       " ('methadone', 0.7679111957550049),\n",
       " ('secobarbital', 0.7673499584197998),\n",
       " ('aspirin', 0.7639237642288208),\n",
       " ('perphenazine', 0.7494533061981201),\n",
       " ('indoprofen', 0.742222011089325),\n",
       " ('codeine', 0.7416385412216187)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.embedding_table.most_similar('ibuprofen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('has', 0.712862491607666),\n",
       " ('previously', 0.45508110523223877),\n",
       " ('recently', 0.45498350262641907),\n",
       " ('examining', 0.41651538014411926),\n",
       " ('had', 0.4008268713951111),\n",
       " ('speculate', 0.395913302898407),\n",
       " ('herein', 0.3913381099700928),\n",
       " ('heretofore', 0.3901556730270386),\n",
       " ('nap', 0.3745071291923523),\n",
       " ('terminology', 0.36553382873535156)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.embedding_table.most_similar('have')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "Learn more about Azure Machine Learning Package for Text Analytics in these articles:\n",
    "\n",
    "+ Read the [package overview and learn how to install it](https://aka.ms/aml-packages/text).\n",
    "\n",
    "+ Explore the [reference documentation](https://aka.ms/aml-packages/text) for this package.\n",
    "\n",
    "+ Learn about [other Python packages for Azure Machine Learning](reference-python-package-overview.md)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
