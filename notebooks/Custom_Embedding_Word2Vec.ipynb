{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AML Package for Text Analytics - Custom Word2Vec Embeddings\n",
    "### Training Word2Vec Embeddings Model on Domain-Specific Data\n",
    "\n",
    "This notebook shows how to use **Azure Machine Learning Package for Text Analytics** (AMLPTA) to train Word2Vec word embeddings on a large text corpus. \n",
    "\n",
    "The trained Word2Vec embeddings can be saved, and reloaded for later use to get embeddings of words or sentences in new datasets, where they will be used as features. \n",
    "\n",
    "A Word2Vec model, trained on a large corpus, learns representation of words that incorporates information about their context. The model is a simple neural network that predicts context words from a target word (skip-gram) or a target word from context words (Continuous Bag of Words CBOW). The first layer of the network acts like a lookup table, converting each input (ie. each word of the dictionary of the corpus) to a vector of fixed size, called embedding vector. This table of word vector representations is learned during training, which is done in an unsupervised fashion -- only text is supplied as input. The embedding table will be extracted after training and used for lookup and word similarity calculations. For example, it is used for lookup in pipelines in which word2vec features are used. \n",
    "\n",
    "In this notebook, a Word2Vec model training is implemented in the class `Word2VecModel`. It consists of the following steps:\n",
    "- `RegExTransformer`, which is used to clean the input text by removing certain patterns (regular expression).\n",
    "- `NltkPreprocessor`, which is used to tockenize and split the input in sentences.\n",
    "- `UngroupTransformer`, which ungroups the detected sentences by writing each one on a row of the dataset.\n",
    "- `Word2VecVectorizer`, which trains the FastText model.\n",
    "\n",
    "Note that `RegExTransformer` is only added to the pipeline if *regex* parameter is not *None* when creating a `Word2VecModel` instance, while `UngroupTransformer` is only added if *detect_sentences = True*.\n",
    "\n",
    "The model is trained on PubMed abstracts, grouped in 18 batches (tsv files of about 1 Gb each). Since the training on the whole data is time-consuming, in the context of this tutorial, training will be performed on a subset of the data.\n",
    "\n",
    "Following are the steps for creating a custom word2vec model using the package:\n",
    "<br> Step 1: Configure and import modules\n",
    "<br> Step 2: Prepare data for modeling and evaluation\n",
    "<br> Step 3: Train the Word2Vec model \n",
    "<br> Step 4: Save and load pipeline for additional training\n",
    "<br> Step 5: Save and load embeddings for lookup \n",
    "\n",
    "Consult the [package reference documentation](https://aka.ms/aml-packages/text) for the detailed reference for each module and class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites \n",
    "\n",
    "1. If you don't have an Azure subscription, create a [free account](https://azure.microsoft.com/free/?WT.mc_id=A261C142F) before you begin.\n",
    "\n",
    "1. The following accounts and application must be set up and installed:\n",
    "   - Azure Machine Learning Experimentation account \n",
    "   - An Azure Machine Learning Model Management account\n",
    "   - Azure Machine Learning Workbench installed\n",
    "\n",
    "   If these three are not yet created or installed, follow the [Azure Machine Learning Quickstart and Workbench installation](../service/quickstart-installation.md) article. \n",
    "\n",
    "1. The Azure Machine Learning Package for Text Analytics must be installed. Learn how to [install this package here](https://aka.ms/aml-packages/text)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Configure and import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Metadata-Version: 2.0\n",
      "Name: azureml-tatk\n",
      "Version: 0.1.18121.30a1\n",
      "Summary: Microsoft Azure Machine Learning Package for Text Analytics\n",
      "Home-page: https://microsoft.sharepoint.com/teams/TextAnalyticsPackagePreview\n",
      "Author: Microsoft Corporation\n",
      "Author-email: amltap@microsoft.com\n",
      "Installer: pip\n",
      "License: UNKNOWN\n",
      "Location: c:\\users\\tatk\\appdata\\local\\amlworkbench\\python\\lib\\site-packages\n",
      "Requires: pyspark, unidecode, dill, sklearn-crfsuite, h5py, scipy, pdfminer.six, azure-storage, scikit-learn, ruamel.yaml, azure-ml-api-sdk, bqplot, ipython, jsonpickle, numpy, matplotlib, ipywidgets, nose, pandas, pytest, docker, validators, qgrid, nltk, gensim, requests, lxml, keras\n",
      "Classifiers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 8.1.2, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Import Packages \n",
    "# Use Azure Machine Learning history magic to control history collection\n",
    "# History is off by default, options are \"on\", \"off\", or \"show\"\n",
    "#%azureml history on\n",
    "%matplotlib inline\n",
    "# Use the Azure Machine Learning data collector to log various metrics\n",
    "from azureml.logging import get_azureml_logger\n",
    "import os\n",
    "import warnings\n",
    "logger = get_azureml_logger()\n",
    "\n",
    "# Log cell runs into run history\n",
    "logger.log('Cell','Set up run')\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.1) Configure AzureML logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<azureml.logging.script_run_request.ScriptRunRequest at 0x1cc693c9eb8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Azure Machine Learning logger and magics for logging and run-history tracking\n",
    "# Use the Azure Machine Learning data collector to log various metrics\n",
    "from azureml.logging import get_azureml_logger\n",
    "logger = get_azureml_logger()\n",
    "\n",
    "# Log cell runs into run history\n",
    "logger.log('Cell','Set up run')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.2) Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'pattern' package not found; tag filters are not available for English\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tatk.utils import models_dir\n",
    "from tatk.pipelines.feature_extraction.word2vec_model import Word2VecModel\n",
    "from tatk.feature_extraction.word2vec_vectorizer import Word2VecVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Prepare data for modeling and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.1) Download and parse Pubmed data. \n",
    "Please download the raw MEDLINE abstract data from [MEDLINE](https://www.nlm.nih.gov/pubs/factsheets/medline.html). The data is publicly available in the form of XML files on their [FTP server](https://ftp.ncbi.nlm.nih.gov/pubmed/baseline). There are 892 XML files available on the server and each of the XML files has the information of 30,000 articles.\n",
    "\n",
    "You should keep 2 columns: pmid (ID of the document) and abstract, and save them in tab-separated format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can split them in multiple files and perform incremental learning, instead of reading in-memory a big amount of data.\n",
    "\n",
    "Set `file_path` to point to the specific dataset to train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r'C:\\Users\\tatk\\Downloads\\pubmed18nbatch#1.tsv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.2) Read the first batch in-memory. \n",
    "Each paper has an ID (pmid) and a text abstract. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(file_path, sep = \"\\t\", usecols = ['pmid', 'abstract'], encoding = \"ISO-8859-1\").dropna()#read in-memory\n",
    "df = data[:5000] # take a subset for faster training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pmid</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4525790</td>\n",
       "      <td>We have developed a cell-free system to study ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4525907</td>\n",
       "      <td>The oxy-form of sickle hemoglobin (Hb S) is ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4525937</td>\n",
       "      <td>Of 17 consecutive patients with acute granuloc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4526201</td>\n",
       "      <td>Carbon magnetic resonance T(1) relaxation and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4526202</td>\n",
       "      <td>The ability of fibroblasts to perform unschedu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      pmid                                           abstract\n",
       "0  4525790  We have developed a cell-free system to study ...\n",
       "1  4525907  The oxy-form of sickle hemoglobin (Hb S) is ab...\n",
       "2  4525937  Of 17 consecutive patients with acute granuloc...\n",
       "3  4526201  Carbon magnetic resonance T(1) relaxation and ...\n",
       "4  4526202  The ability of fibroblasts to perform unschedu..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train the Word2Vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3.1) Create the Word2Vec model pipeline\n",
    "Initialize the pipeline with default parameters. No regular expression cleaning is performed, and sentences are detected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2VecModel::create_pipeline ==> start\n",
      "input_col=abstract\n",
      "input_col=NltkPreprocessor08a5db0dd16644aabf8c7e8812e47549\n",
      "input_col=UngroupTransformerb59cc67a90b34d78bb2e9adb445290d6\n",
      ":: number of jobs for the pipeline : 6\n",
      "0\tnltk_preprocessor\n",
      "1\tungroup_transformer\n",
      "2\tvectorizer\n",
      "Word2VecModel::create_pipeline ==> end\n"
     ]
    }
   ],
   "source": [
    "word2vec_model = Word2VecModel(input_col = 'abstract', regex = None, detect_sentences = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2VecModel TATK Pipeline:\n",
      "0 - nltk_preprocessor(abstract,NltkPreprocessor08a5db0dd16644aabf8c7e8812e47549)\n",
      "1 - ungroup_transformer(NltkPreprocessor08a5db0dd16644aabf8c7e8812e47549,UngroupTransformerb59cc67a90b34d78bb2e9adb445290d6)\n",
      "2 - vectorizer(UngroupTransformerb59cc67a90b34d78bb2e9adb445290d6,Word2VecVectorizerb2f2ed9dea734f5cb2c53909b78e15a6)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(word2vec_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3.2) Display and Change default pipeline parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aggregation_func': <function tatk.feature_extraction.word2vec_vectorizer.Word2VecVectorizer.aggregate_mean(sentence_matrix)>,\n",
       " 'case_sensitive': False,\n",
       " 'context_window_size': 5,\n",
       " 'copy_from_path': True,\n",
       " 'embedding_size': 100,\n",
       " 'embedding_table': None,\n",
       " 'get_from_path': True,\n",
       " 'input_col': 'UngroupTransformerb59cc67a90b34d78bb2e9adb445290d6',\n",
       " 'lr_end': 0.005,\n",
       " 'lr_start': 0.05,\n",
       " 'min_df': 5,\n",
       " 'negative_sample_size': 5,\n",
       " 'num_epochs': 5,\n",
       " 'num_workers': 4,\n",
       " 'output_col': 'Word2VecVectorizerb2f2ed9dea734f5cb2c53909b78e15a6',\n",
       " 'return_type': 'word_vector',\n",
       " 'save_overwrite': True,\n",
       " 'skip_OOV': False,\n",
       " 'trainable': True,\n",
       " 'trained_model': None,\n",
       " 'use_hierarchical_softmax': 0,\n",
       " 'use_skipgram': 0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.get_step_params_by_name('vectorizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change model parameters\n",
    "word2vec_model.set_step_params_by_name('vectorizer', use_skipgram = 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3.3) Fit the model on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2VecModel::fit ==> start\n",
      "schema: col=pmid:I8:0 col=abstract:TX:1 header+\n",
      "NltkPreprocessor::tatk_fit_transform ==> start\n",
      "NltkPreprocessor::tatk_fit_transform ==> end \t Time taken: 0.05 mins\n",
      "UngroupTransformer::tatk_fit_transform ==> start\n",
      "UngroupTransformer::tatk_fit_transform ==> end \t Time taken: 0.0 mins\n",
      "Word2VecVectorizer::tatk_fit ==> start\n",
      "collecting all words and their counts\n",
      "PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "PROGRESS: at sentence #10000, processed 281599 words, keeping 15380 word types\n",
      "PROGRESS: at sentence #20000, processed 557607 words, keeping 20905 word types\n",
      "collected 25746 word types from a corpus of 834512 raw words and 29982 sentences\n",
      "Loading a fresh vocabulary\n",
      "min_count=5 retains 9277 unique words (36% of original 25746, drops 16469)\n",
      "min_count=5 leaves 806233 word corpus (96% of original 834512, drops 28279)\n",
      "deleting the raw counts dictionary of 25746 items\n",
      "sample=0.0001 downsamples 476 most-common words\n",
      "downsampling leaves estimated 391647 word corpus (48.6% of prior 806233)\n",
      "estimated required memory for 9277 words and 100 dimensions: 12060100 bytes\n",
      "resetting layer weights\n",
      "training model with 4 workers on 9277 vocabulary and 100 features, using sg=1 hs=0 sample=0.0001 negative=5 window=5\n",
      "PROGRESS: at 28.93% examples, 560283 words/s, in_qsize 7, out_qsize 0\n",
      "PROGRESS: at 58.57% examples, 569902 words/s, in_qsize 7, out_qsize 0\n",
      "PROGRESS: at 88.25% examples, 572770 words/s, in_qsize 7, out_qsize 0\n",
      "worker thread finished; awaiting finish of 3 more threads\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "training on 4172560 raw words (1957760 effective words) took 3.4s, 572878 effective words/s\n",
      "vocabulary size =9277\n",
      "Word2VecVectorizer::tatk_fit ==> end \t Time taken: 0.07 mins\n",
      "Time taken: 0.11 mins\n",
      "Word2VecModel::fit ==> end\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Word2VecModel(detect_sentences=True, input_col='abstract', regex=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3.4) Script to train the embeddings on multiple batches (note: very long on full 21 Gb of data).\n",
    "We loop over the different batches. Assuming file paths to the batches are (replace below with your path):\n",
    "\n",
    "file_path = r\"C:\\Users\\tatkdocs\\Downloads\\batch_files\\pubmed18nbatch#1.tsv\", r\"C:\\Users\\tatkdocs\\Downloads\\batch_files\\pubmed18nbatch#2.tsv\", etc.\n",
    "\n",
    "We read every file in-memory, and feed it to the model for incremental learning.\n",
    "\n",
    "Replace *False* by *True* if you would like to perform this step. Otherwise, proceed with the model trained above on a subset of batch #1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = 1\n",
    "if False:#False:\n",
    "    word2vec_model = Word2VecModel(input_col = 'abstract', regex = None, detect_sentences = True)#Initialize the model.\n",
    "    for b in range(1, num_batches + 1):\n",
    "        print(b)\n",
    "        file_path = r\"C:\\Users\\tatkdocs\\Downloads\\pubmed18nbatch#{}}.tsv\".format(b)\n",
    "        df = pd.read_csv(file_path, sep = \"\\t\", usecols = ['abstract'], encoding = \"ISO-8859-1\").dropna()\n",
    "        print(df.shape)\n",
    "        word2vec_model.fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Save and load pipeline for additional training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4.1) Save and Load the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseTextModel::save ==> start\n",
      "TatkPipeline::save ==> start\n",
      "saving Word2Vec object under C:\\Users\\tatk\\tatk\\resources\\models\\word2vec_model\\pipeline\\vectorizer\\embedding_model.gen, separately None\n",
      "not storing attribute syn0norm\n",
      "not storing attribute cum_table\n",
      "saved C:\\Users\\tatk\\tatk\\resources\\models\\word2vec_model\\pipeline\\vectorizer\\embedding_model.gen\n",
      "Time taken: 0.0 mins\n",
      "TatkPipeline::save ==> end\n",
      "Time taken: 0.0 mins\n",
      "BaseTextModel::save ==> end\n",
      "BaseTextModel::load ==> start\n",
      "TatkPipeline::load ==> start\n",
      "loading Word2Vec object from C:\\Users\\tatk\\tatk\\resources\\models\\word2vec_model\\pipeline\\vectorizer\\embedding_model.gen\n",
      "loading wv recursively from C:\\Users\\tatk\\tatk\\resources\\models\\word2vec_model\\pipeline\\vectorizer\\embedding_model.gen.wv.* with mmap=None\n",
      "setting ignored attribute syn0norm to None\n",
      "setting ignored attribute cum_table to None\n",
      "loaded C:\\Users\\tatk\\tatk\\resources\\models\\word2vec_model\\pipeline\\vectorizer\\embedding_model.gen\n",
      "Word2VecVectorizer: Word2Vec model loaded from C:\\Users\\tatk\\tatk\\resources\\models\\word2vec_model\\pipeline\\vectorizer\\embedding_model.gen\n",
      "Time taken: 0.0 mins\n",
      "TatkPipeline::load ==> end\n",
      "Time taken: 0.0 mins\n",
      "BaseTextModel::load ==> end\n"
     ]
    }
   ],
   "source": [
    "pipeline_path = os.path.join(models_dir, 'word2vec_model')\n",
    "word2vec_model.save(pipeline_path, create_folders_on_path=True)\n",
    "word2vec_model2 = Word2VecModel.load(pipeline_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4.2) Perform additional training on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2VecModel::fit ==> start\n",
      "schema: col=pmid:I8:0 col=abstract:TX:1 header+\n",
      "NltkPreprocessor::tatk_fit_transform ==> start\n",
      "NltkPreprocessor::tatk_fit_transform ==> end \t Time taken: 0.01 mins\n",
      "UngroupTransformer::tatk_fit_transform ==> start\n",
      "UngroupTransformer::tatk_fit_transform ==> end \t Time taken: 0.0 mins\n",
      "Word2VecVectorizer::tatk_fit ==> start\n",
      "collecting all words and their counts\n",
      "PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "collected 11942 word types from a corpus of 181960 raw words and 7167 sentences\n",
      "Updating model with new vocabulary\n",
      "New added 3704 unique words (23% of original 15646) and increased the count of 3704 pre-existing words (23% of original 15646)\n",
      "deleting the raw counts dictionary of 11942 items\n",
      "sample=0.0001 downsamples 1046 most-common words\n",
      "downsampling leaves estimated 149681 word corpus (89.2% of prior 167794)\n",
      "estimated required memory for 7408 words and 100 dimensions: 9630400 bytes\n",
      "updating layer weights\n",
      "training model with 4 workers on 9620 vocabulary and 100 features, using sg=1 hs=0 sample=0.0001 negative=5 window=5\n",
      "worker thread finished; awaiting finish of 3 more threads\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "training on 909800 raw words (408078 effective words) took 0.7s, 562449 effective words/s\n",
      "vocabulary size =9620\n",
      "Word2VecVectorizer::tatk_fit ==> end \t Time taken: 0.02 mins\n",
      "Time taken: 0.03 mins\n",
      "Word2VecModel::fit ==> end\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Word2VecModel(detect_sentences=True, input_col='abstract', regex=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.DataFrame(data[5000:6000])\n",
    "word2vec_model2.fit(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Save and load embeddings for lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5.1) Save the embeddings from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2VecVectorizer::save_embeddings ==> start\n",
      "storing 9620x100 projection weights into C:\\Users\\tatk\\tatk\\resources\\models\\word2vec_embeddings.txt\n",
      "Time taken: 0.01 mins\n",
      "Word2VecVectorizer::save_embeddings ==> end\n"
     ]
    }
   ],
   "source": [
    "# Saved embeddings file is in textual format and is readable if opened with a text editor\n",
    "embeddings_file_path = os.path.join(models_dir, 'word2vec_embeddings.txt')\n",
    "word2vec_model2.save_embeddings(embeddings_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5.2) Load the embeddings to memory with include_unk set to True to add OOV treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2VecVectorizer::load_embeddings ==> start\n",
      "loading projection weights from C:\\Users\\tatk\\tatk\\resources\\models\\word2vec_embeddings.txt\n",
      "loaded (9620, 100) matrix from C:\\Users\\tatk\\tatk\\resources\\models\\word2vec_embeddings.txt\n",
      "Time taken: 0.02 mins\n",
      "Word2VecVectorizer::load_embeddings ==> end\n"
     ]
    }
   ],
   "source": [
    "vectorizer = Word2VecVectorizer.load_embeddings(embeddings_file_path, include_unk = True,\n",
    "                                                unk_method = 'rnd', unk_vector = None, unk_word = '<UNK>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5.3) Embedding Lookup: Get word and subword indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2VecVectorizer::tatk_transform ==> start\n",
      "Word2VecVectorizer::tatk_transform ==> end \t Time taken: 0.0 mins\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>indices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have fever</td>\n",
       "      <td>[94, 55, 1078]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My doctor prescribed me ibuprofen.</td>\n",
       "      <td>[9620, 8025, 9620, 4657, 9620]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 text                         indices\n",
       "0                        I have fever                  [94, 55, 1078]\n",
       "1  My doctor prescribed me ibuprofen.  [9620, 8025, 9620, 4657, 9620]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_predict = pd.DataFrame({'text' : [\"I have fever\", \"My doctor prescribed me ibuprofen.\"]})\n",
    "vectorizer.input_col = 'text'\n",
    "vectorizer.output_col = 'indices'\n",
    "vectorizer.return_type = 'word_index'\n",
    "result = vectorizer.tatk_transform(df_predict)\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5.4) Embedding Lookup: Get word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2VecVectorizer::tatk_transform ==> start\n",
      "Word2VecVectorizer::tatk_transform ==> end \t Time taken: 0.0 mins\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>indices</th>\n",
       "      <th>word_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have fever</td>\n",
       "      <td>[94, 55, 1078]</td>\n",
       "      <td>[[0.175807997584, -0.01292799972, 0.2981559932...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My doctor prescribed me ibuprofen.</td>\n",
       "      <td>[9620, 8025, 9620, 4657, 9620]</td>\n",
       "      <td>[[-0.118047584745, -0.0765161739483, -0.008675...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 text                         indices  \\\n",
       "0                        I have fever                  [94, 55, 1078]   \n",
       "1  My doctor prescribed me ibuprofen.  [9620, 8025, 9620, 4657, 9620]   \n",
       "\n",
       "                                         word_vector  \n",
       "0  [[0.175807997584, -0.01292799972, 0.2981559932...  \n",
       "1  [[-0.118047584745, -0.0765161739483, -0.008675...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vectorizer.output_col = 'word_vector'\n",
    "vectorizer.return_type = 'word_vector'\n",
    "result = vectorizer.tatk_transform(df_predict)\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5.5) Embedding Lookup: Get sentence embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2VecVectorizer::tatk_transform ==> start\n",
      "Word2VecVectorizer::tatk_transform ==> end \t Time taken: 0.0 mins\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>indices</th>\n",
       "      <th>word_vector</th>\n",
       "      <th>sentence_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have fever</td>\n",
       "      <td>[94, 55, 1078]</td>\n",
       "      <td>[[0.175807997584, -0.01292799972, 0.2981559932...</td>\n",
       "      <td>[-0.198054343462, -0.0625083340953, 0.09954333...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My doctor prescribed me ibuprofen.</td>\n",
       "      <td>[9620, 8025, 9620, 4657, 9620]</td>\n",
       "      <td>[[-0.118047584745, -0.0765161739483, -0.008675...</td>\n",
       "      <td>[-0.0933727497029, -0.113094106418, -0.0371973...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 text                         indices  \\\n",
       "0                        I have fever                  [94, 55, 1078]   \n",
       "1  My doctor prescribed me ibuprofen.  [9620, 8025, 9620, 4657, 9620]   \n",
       "\n",
       "                                         word_vector  \\\n",
       "0  [[0.175807997584, -0.01292799972, 0.2981559932...   \n",
       "1  [[-0.118047584745, -0.0765161739483, -0.008675...   \n",
       "\n",
       "                                     sentence_vector  \n",
       "0  [-0.198054343462, -0.0625083340953, 0.09954333...  \n",
       "1  [-0.0933727497029, -0.113094106418, -0.0371973...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vectorizer.output_col = 'sentence_vector'\n",
    "vectorizer.return_type = 'sentence_vector'\n",
    "result = vectorizer.tatk_transform(df_predict)\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5.6) Embedding Lookup: Get most similar word to a given word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('hay', 0.7872635126113892),\n",
       " ('haemorrhagic', 0.7152248620986938),\n",
       " ('era', 0.7067720890045166),\n",
       " ('chikungunya', 0.6999553442001343),\n",
       " ('southern', 0.6974883675575256),\n",
       " ('dairy', 0.695317804813385),\n",
       " ('encephalitis', 0.6922379732131958),\n",
       " ('vomiting', 0.6884969472885132),\n",
       " ('yellow', 0.6828291416168213),\n",
       " ('diarrhea', 0.681731104850769)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.embedding_table.most_similar('fever')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('prognosis', 0.9426259994506836),\n",
       " ('facilities', 0.941356360912323),\n",
       " ('radiotherapy', 0.9408228993415833),\n",
       " ('grounds', 0.9350035190582275),\n",
       " ('papers', 0.9279524087905884),\n",
       " ('communicable', 0.9261314868927002),\n",
       " ('emphasized', 0.9256585836410522),\n",
       " ('tasks', 0.9232277870178223),\n",
       " ('achievements', 0.9187901616096497),\n",
       " ('personnel', 0.9185605645179749)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.embedding_table.most_similar('doctor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('chlormethiazole', 0.940321683883667),\n",
       " ('dipropionate', 0.9348151087760925),\n",
       " ('beclomethasone', 0.9326766133308411),\n",
       " ('benorylate', 0.9220473766326904),\n",
       " ('f2alpha', 0.9094793200492859),\n",
       " ('phosphide', 0.9047806859016418),\n",
       " ('bx24', 0.8957622051239014),\n",
       " ('frusemide', 0.8943196535110474),\n",
       " ('thrice', 0.8939546942710876),\n",
       " ('ra27', 0.8856890201568604)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.embedding_table.most_similar('ibuprofen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('has', 0.6336463689804077),\n",
       " ('been', 0.6312592029571533),\n",
       " ('examining', 0.579426646232605),\n",
       " ('vertebrates', 0.5628061294555664),\n",
       " ('observing', 0.5623247623443604),\n",
       " ('microscopically', 0.5592154860496521),\n",
       " ('localize', 0.5555355548858643),\n",
       " ('call', 0.5527269840240479),\n",
       " ('characterize', 0.5492762923240662),\n",
       " ('technic', 0.5434949994087219)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.embedding_table.most_similar('have')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "Learn more about Azure Machine Learning Package for Text Analytics in these articles:\n",
    "\n",
    "+ Read the [package overview and learn how to install it](https://aka.ms/aml-packages/text).\n",
    "\n",
    "+ Explore the [reference documentation](https://aka.ms/aml-packages/text) for this package.\n",
    "\n",
    "+ Learn about [other Python packages for Azure Machine Learning](reference-python-package-overview.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Â© 2018 Microsoft. All rights reserved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
